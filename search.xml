<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Centos7防火墙设置]]></title>
    <url>%2F2019%2F08%2F05%2FCentos7%E9%98%B2%E7%81%AB%E5%A2%99%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[CentOS 7中使用的防火墙为firewall，在CentOS 6.5中在iptables防火墙中进行了升级了。 用于实现持久的网络流量规则。 可以动态修改单条规则，动态管理规则集，允许更新规则而不破坏现有会话和连接 使用区域(zone)和服务(service) 默认是拒绝的，需要设置以后才能放行 CentOS 7放行端口需要在云服务器管理后台和服务器firewall同时设置才有效！注意默认全部拒绝！管理后台通过权限组添加规则。 配置目录/usr/lib/firewalld/services 目录中存放定义好的网络服务和端口参数，系统参数。配置时引用服务名称 /etc/firewalld/为配置目录，使用区域在zones下 常用命令firewall-cmd：是Linux提供的操作firewall的一个工具。 （1）firewalld服务启动、关闭、重启，设置开机自启1234567891011121314151617# 启动systemctl start firewalld # 关闭systemctl stop firewalld # 重启systemctl restart firewalld # 开机启动 systemctl enable firewalld # 取消开机启动systemctl disable firewalld # 查看是否开机自启systemctl is-enabled firewalld （2）添加端口/服务。用户可以通过修改配置文件的方式添加端口，也可以通过命令的方式添加端口，注意，修改的内容会在/etc/firewalld/ 目录下的配置文件中还体现。例如在public区域添加tcp端口80201firewall-cmd --zone=public --permanent --add-port=8010/tcp –permanent：表示设置为持久； –add-port：标识添加的端口； 可以在/etc/firewalld/zones/public.xml中看见已经添加成功。 这个配置文件也可以手动修改，如添加服务123vi /etc/firewalld/zones/public.xmli加入 &lt;service name=&quot;mysql&quot;/&gt; 也可以通过--add-service添加，1firewall-cmd --permanent --zone=public --add-service=mysql 重载可生效firewall-cmd --reload。 （3）查看规则1firewall-cmd --list-all （4）其它1234567891011121314151617181920# 查看防火墙状态firewall-cmd --state # 查看默认的域firewall-cmd --get-default-zone# 查看所有的域firewall-cmd --get-zones# 查看所有域的信息firewall-cmd --list-all-zones# 查看指定域的信息firewall-cmd --zone=public --list-all# 查看可以添加的服务firewall-cmd --get-services# 设置指定域为默认域firewall-cmd --set-default-zone=public]]></content>
      <tags>
        <tag>CentOS7</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos 7利用yum安装apache+mysql 8+php7 一篇搞定！]]></title>
    <url>%2F2019%2F08%2F04%2FCentos-7%E5%88%A9%E7%94%A8yum%E5%AE%89%E8%A3%85apache-mysql-8-php7%2F</url>
    <content type="text"><![CDATA[本篇文章主要为大家详细介绍了centos7利用yum配置php环境的详细步骤，带你过坑~安装的mysql版本为Ver 8.0.17 for Linux on x86_64 (MySQL Community Server - GPL)安装的php版本为PHP 7.0.33 为了方便大家复制code，本篇文章关闭复制自动追加版权。 mysql安装1. 清理原来的mysql文件(1)查看是否有已安装的mysql版本1rpm -qa | grep mysql 可能的显示：mysql-community-libs-8.0.12-1.el7.x86_64mysql80-community-release-el7-1.noarchmysql-community-client-8.0.12-1.el7.x86_64mysql-community-common-8.0.12-1.el7.x86_64mysql-community-server-8.0.12-1.el7.x86_64 如果有的话，通过下面的命令进行删除1yum remove mysql-xxx-xxx- (2)查找mysql的相关文件（如有建议删除）1find / -name mysql (3)根据自身的需求决定是否删除mysql的配置文件信息1rm -rf /var/lib/mysql (4)查找并删除数据库mariadb1rpm -qa | grep mariadb 可能出现的结果mariadb-libs-5.5.56-2.el7.x86_64 将查找出来的结果进行强制删除1rpm -e --nodeps mariadb-libs-5.5.56-2.el7.x86_64 2. 下载安装mysql(1)进入mysql yum源选择linux7跳转， 不管login和sign up，右键点击下方的No thanks, just start my download.选择复制链接地址 选择下载存放的地址，比如1cd /usr/local (2)使用复制的链接地址yum源，如 https://dev.mysql.com/get/mysql80-community-release-el7-3.noarch.rpm1wget https://dev.mysql.com/get/mysql80-community-release-el7-3.noarch.rpm (3)安装yum源1yum localinstall mysql80-community-release-el7-3.noarch.rpm (4)更新yum源12yum clean allyum makecache (5)开始安装MySQL1yum install mysql-community-server (6)启动MySQL1systemctl start mysqld 3. 更改mysql默认的随机密码启动成功后可以查看初始化密码随机生成的1cat /var/log/mysqld.log | grep password 登录MySQL，输入刚刚的随机密码12mysql -u root -pEnter password: 更改密码，密码要求必须包含大小写字母数字及一个特殊字符！1mysql&gt; ALTER USER &apos;root&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;新密码&apos;; 4. 远程连接设置12mysql&gt; use mysql;mysql&gt; update user set host=&apos;%&apos; where user=&apos;root&apos;; 授权用户名的权限，赋予任何主机访问数据的权限12mysql&gt; GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; WITH GRANT OPTION;mysql&gt; FLUSH PRIVILEGES; 注意：centos7需要在云服务器管理后台和服务器防火墙都要开启mysql服务端口，默认为3306，防火墙配置方式查看12firewall-cmd --permanent --zone=public --add-service=mysqlfirewall-cmd --reload 可通过远程连接工具Navicat等连接尝试，如连接失败2003... unknown error 1006则为防火墙问题，其他问题可以尝试更改/etc/my.cnf再重启1234567[mysqld]# 表示允许任何主机登陆MySQLbind-address = 0.0.0.0port=3306default-authentication-plugin=mysql_native_password apache安装（1）安装之前先检查一下系统是否有默认安装的apache1rpm -qa | grep httpd 有的话，删除1rpm -e * * * *(包名) （2）安装1234567891011121314# 更新yum update# 安装必备的包yum -y install gcc gcc-c++ make# 安装apacheyum install httpd# apache 服务开启systemctl start httpd# 设置apache开机启动systemctl enable httpd （3）配置防火墙，开启80端口。注意：centos7需要在云服务器管理后台和服务器防火墙都要开启http服务端口，默认为80，防火墙配置方式查看12firewall-cmd --permanent --zone=public --add-service=httpfirewall-cmd --reload （4）现在通过服务器ip:80端口访问，就可以看见默认的apache界面就算成功。 如果不行，可尝试更改vi /etc/httpd/conf/httpd.conf 配置文件再重启1ServerName localhost:80 php安装（1）查看是否有以前的文件，yum默认安装的不是7.0，如果有尝试，需要完全删除以前的版本1rpm -qa|grep php 可能有如下的包php-pdo-5.1.6-27.el5_5.3php-mysql-5.1.6-27.el5_5.3php-xml-5.1.6-27.el5_5.3php-cli-5.1.6-27.el5_5.3php-common-5.1.6-27.el5_5.3php-gd-5.1.6-27.el5_5.3 如果有的话需要删除。注意包之间可能会有互相依赖，先删除无依赖的1rpm -e * * * *(包名) （2）安装。更改yum源12rpm -Uvh https://mirror.webtatic.com/yum/el7/webtatic-release.rpm yum -y install php70w.x86_64 php70w-cli.x86_64 php70w-common.x86_64 php70w-gd.x86_64 php70w-ldap.x86_64 php70w-mbstring.x86_64 php70w-mysql.x86_64 php70w-pdo.x86_64 php70w-pear.noarch php70w-process.x86_64 php70w-xml.x86_64 php70w-xmlrpc.x86_64 php环境测试，在/var/www/html目录下，vi index.php输入如下内容保存。123&lt;?php phpinfo();?&gt; 重启httpd服务，systemctl restart httpd后查看ip:80/index.php，出现如下界面就算成功。 如果原样输出，或者直接下载则失败，失败检查可尝试更改/etc/httpd/conf/httpd.conf重启123456&lt;IfModule mime_module&gt;....AddType application/x-compress .ZAddType application/x-gzip .gz .tgz下添加AddType application/x-httpd-php .php]]></content>
      <tags>
        <tag>CentOS7</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[国外VPS云服务器购买及ssr搭建]]></title>
    <url>%2F2019%2F08%2F04%2F%E5%9B%BD%E5%A4%96VPS%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%B4%AD%E4%B9%B0%E5%8F%8Assr%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[是否需要买一个云主机？VPS云？当前常见的云主机管理平台包括Eucalyptus云主机，CloudStack云主机，OpenStack云主机 , OpenNebula云主机，onapp云主机等。国外的服务器有什么好处？简单来说不需要备案，搭个ssr免了买vpn的钱。 国外VPS选购SSR的速度完全取决于你VPS速度的快慢，国外VPS商家众多，本篇文章主要推荐vultr和搬瓦工这两家。 1.vultr 目前在全球有16个机房（包括日本、新加坡、美国等），基于KVM虚拟，纯SSD硬盘，后台允许购买Windows系统的VPS，支持Alipay（支付宝等）、PayPal等。官方网站：www.vultr.com点击Vultr注册充值送50美金点击进入，注册完成点击Products，选择deploy new server选择合适的机器。充值选择Billing，从链接进入会看见充值送50美刀 2.搬瓦工 KVM虚拟，支持Centos, Debian, Ubuntu，SSD硬盘，每个VPS自带一个独立IPv4，可选机房有四个。 点击注册搬瓦工，注册成功后点击VPS Hosting选择。 服务器连接以vultr为例，选购完成后在Products能看见等待设备安装，点击进入设备可以看见ip，username和password，默认的防火墙是打开ssh 22端口的，我们可以通过远程登录管理，小编使用的xshell，普通的xshell是试用期30天，小编这里收集了一个Free for home/school版本的，如果链接失效可以联系我 链接：https://pan.baidu.com/s/1ARlnKwUBWz9wQocCzmEU1Q提取码：50qf复制这段内容后打开百度网盘手机App，操作更方便哦 安装完后选择文件-&gt;新建，设置主机名，即为vultr后台的ip address。点击用户身份验证，填写用户名为vultr的username，密码为vultr的password，点击确定。会话创建完成，双击就可以连接，提示密码保存的窗口就点击同意，不然下次还得输入密码。 SSR搭建连接后，在想要放置的位置开始搭建ssr。1234567$ yum install wget -y$ wget --no-check-certificate https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocksR.sh$ chmod +x shadowsocksR.sh$ ./shadowsocksR.sh 2&gt;&amp;1 | tee shadowsocksR.log 运行后，会提示你设置端口（port）和密码（password），加密方式,协议,混淆等 （1）设置SSR连接密码：Please enter password for ShadowsocksR。（如不设定，默认为 teddysun.com）（2）设置你的服务器端口[1-65535]：Please enter a port for ShadowsocksR 。（如不设定就是提示的）（3）设置加密方式：Please select stream cipher for ShadowsocksR，输入序号回车选择：（4）选择协议：Please select protocol for ShadowsocksR，输入序号回车选择：（如不设定，默认为 origin）（5）选择混淆方式：Please select obfs for ShadowsocksR，输入序号回车选择(免流请选择2:http_simple),（如不设定，默认为 plain）设置完成回车，等待几分钟完成， 还需要在服务器及vultr后台开放端口！如 端口为8502123456# 在public.xml添加端口firewall-cmd --zone=public --permanent --add-port=8502/tcp firewall-cmd --zone=public --permanent --add-port=8502/udp # 重启防火墙生效systemctl restart firewalld.service vultr后台配置防火墙端口Products-&gt;firewall。没有权限组的就创建一个，有的话进入点击编辑Edit firewall，选择TCP，填写刚刚设置ssr的端口，点击加号添加提示大约120S内生效。 SSR各系统连接工具下载请看https://github.com/shadowsocks，选择合适的系统点击release选择版本下载。 ssr多端口配置脚本默认创建单用户配置文件，如需配置多用户，需更改配置文件。 ssr服务启动/停止/重启/状态：/etc/init.d/shadowsocks start/stop/restart/status 编辑配置文件：vi /etc/shadowsocks.json 按i进行编辑将原来的server_port和password改为port_password对应的多个以port为键，password为值形式的Esc :wq保存退出。重启shadowsocks服务。 注意：新增加的端口号port仍然需要在防火墙配置入站端口规则。]]></content>
      <tags>
        <tag>VPS</tag>
        <tag>ssr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Google Play应用上架流程]]></title>
    <url>%2F2019%2F08%2F02%2FGoogle-Play%E5%BA%94%E7%94%A8%E4%B8%8A%E6%9E%B6%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[前言柠檬爱玩小游戏的小编喜欢玩2048，迫于市面上的小游戏广告太多了😳，机智的小编就用cocos creator写了个小游戏，想要发布到线上给大家一起玩呀😁，开始放到github page，地址为https://luohongxfb.github.io/2048/ （移动端打开），但是网不好可咋整啊~ 机智的小编打包成apk了，因为小编不想要倒腾买软著，又费时间的又费钱😂，小编就想到发布到google play啦 Cocos Creator打包到原生平台需要先配置原生开发环境，安装jdk，下载Android Studio并且下载需要的sdk和ndk，具体查看官方文档：https://docs.cocos.com/creator/manual/zh/publish/setup-native-development.html 。打包发布原生平台查看官方文档：https://docs.cocos.com/creator/manual/zh/publish/publish-native.html 前提：科学上网；一张Visa/Master卡，开通账号需要支付25美刀。 2048 Google play地址为：https://play.google.com/store/apps/details?id=com.aililuo.simple2048 获取一个开发者账号 1. 注册google账号（如已经有了可跳过这步直接登录）：https://accounts.google.com/SignUp推荐使用gmail，如果想用自己的邮箱点击-&gt;改用我的当前电子邮件地址。填写完成之后点击下一步，同意协议，验证手机号、邮箱之后，可以登录啦~账号首页地址为https://myaccount.google.com/ 2.开通开发者权限：https://play.google.com/apps/publish/signup/勾选同意协议，点击继续付款绑定信用卡，支付25$就可以了。注意：可能会多扣了1$，稍等会返还的。 支付完成，填写账号的详细信息，点击完成注册就可以愉快的上传应用啦。 应用创建前面的都完成了，访问https://play.google.com/apps/publish/ ，应该能看见如下的界面，点击创建应用选择app的默认语言，填写app名称，点击创建即可进入详情页详情页可以看见左侧一些带灰色勾勾的项，没错，就是全部点亮✅就可以发布了 默认一创建应用是跳转的商品详情界面的，但是需要apk信息及内容分级，因各步骤互相关联，建议按下列的步骤来。 1. 应用版本 选择合适的版本，比如当前我们要发布当然要选正式渠道，点击后面的管理-&gt;创建版本。选择合适的方式管理应用签名密钥，因为当前发布的小游戏没有什么三方，小编就直接选择推荐的Google管理了，点击继续，如果需要保持原来的签名选择停用。上传apk/app bundle，填写版本名称，版本的新功能，点击保存 2. 内容分级 填写电子邮箱，选择应用的类别开始填写应用内有无暴力、令人恐惧的内容等，2048选否就对了填写完成，点击保存调查问卷，再点击判断分级，确定分级 3. 商品详情 需要填写app名称、简短的英文说明、详细的中文说明。接下来就是图片资源 要求 图标： 512x512 32位PNG 文件上限1024KB 形状：完整正方形 - Google Play 会动态处理遮盖部分。半径相当于图标大小的 20%，系统已禁止使用透明背景， 阴影：无 - Google Play 会动态处理阴影 查看设计规范详情 屏幕截图 ：JPEG 或 24 位 PNG（无 alpha 透明层）。边长下限：320 像素；边长上限：3840 像素。总的来说，至少需要提供 2 张屏幕截图。每种类型最多可提供 8 张屏幕截图。 置顶大图：宽 1024 x 高 500 JPG 或 24 位 PNG（无 alpha 通道）格式 填写应用类型类别及电子邮件。 填写隐私权政策，当您的应用内没有任何权限申请时，您可以勾选暂不提交隐私权政策网址，有权限申请时，必须要填写 点击保存草稿 4. 定价和分发范围 选定应用是付费还是免费，上架2048当然是免费啦，选择可下载的地区（默认为所有地区都不可下载！）、是否包含广告，2048当然选择否啦最后勾选内容准则 美国出口法律，点击保存草稿 5. 应用内容 填写目标受众群体和内容进入下面步骤选择目标年龄段，点击下一步选择是否会吸引儿童，点击下一步点击提交 应用发布当上面的都填完了，应用的状态会从“草稿”变成“可以发布”，可以在应用列表看见，也能在详情右上方的tab看见。这时候进入应用版本-&gt;正式版渠道 修改版本-&gt;查看-&gt;开始发布正式版 基本上第一次上架新的应用会比较慢审核，小编可是花了三天呢，2048 Google play地址为：https://play.google.com/store/apps/details?id=com.aililuo.simple2048 注意1. 应用签名因为当前上架的2048并没有三方等，所以选的GooglePlay签名计划，上传的应用会被GooglePlay进行二次签名，这样导致的问题就是例如微信、百度定位等需要依赖签名的MD5值或者SHA1值的三方会出现校验失败。 如果需要保持原来的应用签名，在上架时选择停用 2. 隐私权政策当前的2048不需要任何权限，但是一般上架的应用都会有权限的，就必须要填写隐私权政策，关于使用特定权限的其他要求： 活动 要求 您的应用清单请求“通话记录”权限组（例如 READ_CALL_LOG、WRITE_CALL_LOG、ROCESS_OUTGOING_CALLS） 必须由用户主动将应用注册为设备的默认电话或辅助处理程序。 您的应用清单请求“短信”权限组（例如 READ_SMS、SEND_SMS、WRITE_SMS、RECEIVE_SMS、RECEIVE_WAP_PUSH、RECEIVE_MMS） 必须由用户主动将应用注册为设备的默认短信或辅助处理程序。 如果应用中使用了照相，通讯录，电话，短信，外部内存读写，位置等涉及用户隐私的权限，需要先生成隐私政策url，然后先将隐私权限嵌入到App中重写打包上传，还需要在Google play应用的商品详情-&gt;隐私权政策填写隐私权政策url。上图为app内部界面，需要在注册部分或者直接弹窗提示用户授予。生成隐私权url方式： https://www.iubenda.com/blog/privacy-policy-for-android-app/ ：点击操作步骤查看 https://www.privacypolicies.com/ ：点击创建 可以从网上搜一下app隐私声明模板，然后挂在官网上，这个链接也可以用来Google play审核]]></content>
  </entry>
  <entry>
    <title><![CDATA[redis-migrate-tool使用详解]]></title>
    <url>%2F2019%2F08%2F01%2Fredis-migrate-tool%E4%BD%BF%E7%94%A8%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[前言redis-migrate-tool 是维品会开源的一款redis数据迁移工具，基于redis复制，快速，稳定，github地址为：https://github.com/vipshop/redis-migrate-tool 。 快速。 多线程。 基于redis复制。 实时迁移。 迁移过程中，源集群不影响对外提供服务。 异构迁移。 支持Twemproxy集群，redis cluster集群，rdb文件 和 aof文件。 过滤功能。 当目标集群是Twemproxy，数据会跳过Twemproxy直接导入到后端的redis。 迁移状态显示。 完善的数据抽样校验(-C redis_check)。 划重点 实时迁移 迁移过程中，源集群不影响对外提供服务 安装redis-migrate-tool依赖1$ yum -y install automake libtool autoconf bzip2 git 构建12345$ cd redis-migrate-tool$ autoreconf -fvi$ ./configure$ make$ src/redis-migrate-tool -h 警告 在运行工具之前，确保源redis所在的机器有足够的内存可以允许至少一个redis生成.rdb文件，如果源机器有大量足够的内存允许所有的redis生成.rdb，可以在配置文件rmt.conf设置source_safe: false。 下列命令不支持传播给target redis组，因为这些命令下的keys可能交叉了不同的目标redis节点。1RENAME,RENAMENX,RPOPLPUSH,BRPOPLPUSH,FLUSHALL,FLUSHDB,BITOP,MOVE,GEORADIUS,GEORADIUSBYMEMBER,EVAL,EVALSHA,SCRIPT,PFMERGE redis-migrate-tool 命令详解出现下列帮助说明表示安装成功1234567891011121314151617181920212223242526272829303132This is redis-migrate-tool-0.1.0Usage: redis-migrate-tool [-?hVdIn] [-v verbosity level] [-o output file] [-c conf file] [-C command] [-f source address] [-t target address] [-p pid file] [-m mbuf size] [-r target role] [-T thread number] [-b buffer size]Options: -h, --help : this help -V, --version : show version and exit -d, --daemonize : run as a daemon -I, --information : print some useful information -n, --noreply : don&apos;t receive the target redis reply -v, --verbosity=N : set logging level (default: 5, min: 0, max: 11) -o, --output=S : set logging file (default: stderr) -c, --conf-file=S : set configuration file (default: rmt.conf) -p, --pid-file=S : set pid file (default: off) -m, --mbuf-size=N : set mbuf size (default: 512) -C, --command=S : set command to execute (default: redis_migrate) -r, --source-role=S : set the source role (default: single, you can input: single, twemproxy or redis_cluster) -R, --target-role=S : set the target role (default: single, you can input: single, twemproxy or redis_cluster) -T, --thread=N : set how many threads to run the job(default: 4) -b, --buffer=S : set buffer size to run the job (default: 140720309534720 byte, unit:G/M/K) -f, --from=S : set source redis address (default: 127.0.0.1:6379) -t, --to=S : set target redis group address (default: 127.0.0.1:6380) -s, --step=N : set step (default: 1)Commands: redis_migrate : Migrate data from source group to target group. redis_check : Compare data between source group and target group. Default compare 1000 keys. You can set a key count behind. redis_testinsert : Just for test! Insert some string, list, set, zset and hash keys into the source redis group. Default 1000 keys. You can set key type and key count behind. 部分指令解析： -h, --help：帮助 -V, --version：显示版本 -d, --daemonize：后台进程运行 -I, --information：打印一些有用的信息，包括可以解析的指令（126个），不支持的指令（14个）等等 -v, --verbosity=N：设置日志等级。(默认: 5, 最低: 0, 最高: 11) -o, --output=S：设置输出的日志文件 -c, --conf-file=S：设置配置文件。(默认: rmt.conf) -C, --command=S：设置运行的指令(默认: redis_migrate ，迁移)。redis_check 比较源和目的，默认1000个样本key。redis_testinsert测试插入Keys，默认所有类型总共1000个。 -T, --thread=N：设置多少个线程用来运行工具。(默认: 4) 1. 运行迁移1$ src/redis-migrate-tool -c rmt.conf -o log -d 注意：-d指定为后台运行，如果再次运行可能需要杀死占用当前端口的进程。netstat -tnulp查看找到redis-migrate-tool的端口号，kill -9 [端口号]杀死再运行。 指定输出日志文件为log，可通过tail -200 log等查看日志。 2. 抽样检查1234567891011$ src/redis-migrate-tool -c rmt.conf -o log -C redis_checkCheck job is running...Checked keys: 1000Inconsistent value keys: 0Inconsistent expire keys : 0Other check error keys: 0Checked OK keys: 1000All keys checked OK!Check job finished, used 1.041s 抽样检查源组和目标组的数据，默认为1000个。如果需要检查更多的数据，1234567891011$ src/redis-migrate-tool -c rmt.conf -o log -C &quot;redis_check 200000&quot;Check job is running...Checked keys: 200000Inconsistent value keys: 0Inconsistent expire keys : 0Other check error keys: 0Checked OK keys: 200000All keys checked OK!Check job finished, used 11.962s 3. 测试插入一些数据123456789101112$ src/redis-migrate-tool -c rmt.conf -o log -C &quot;redis_testinsert&quot;Test insert job is running...Insert string keys: 200Insert list keys : 200Insert set keys : 200Insert zset keys : 200Insert hash keys : 200Insert total keys : 1000Correct inserted keys: 1000Test insert job finished, used 0.525s 默认插入的数据为string、list、set、zset、hash各200个（均分），总共1000个。如果需要插入更多的键，123456789101112$ src/redis-migrate-tool -c rmt.conf -o log -C &quot;redis_testinsert 30000&quot;Test insert job is running...Insert string keys: 6000Insert list keys : 6000Insert set keys : 6000Insert zset keys : 6000Insert hash keys : 6000Insert total keys : 30000Correct inserted keys: 30000Test insert job finished, used 15.486s 如果只想插入string类型的键（1000个），1$ src/redis-migrate-tool -c rmt.conf -o log -C &quot;redis_testinsert string&quot; 如果想指定插入的几种类型，并且指定总数，123456789101112$src/redis-migrate-tool -c rmt.conf -o log -C &quot;redis_testinsert string|set|list 10000&quot;Test insert job is running...Insert string keys: 3336Insert list keys : 3336Insert set keys : 3328Insert zset keys : 0Insert hash keys : 0Insert total keys : 10000Correct inserted keys: 10000Test insert job finished, used 5.539s 插入校验生成的数据并不会清除，测试时可以尽量减少插入的key。 rmt.conf配置文件配置文件包含三部分：[source], [target] 和 [common] 迁移工具的来源（source）可以是：单独的redis实例，twemproxy集群，redis cluster，rdb文件，aof文件。迁移工具的目标（target）可以是：单独的redis实例，twemproxy集群，redis cluster，rdb文件。 [source]/[target]： type： single：单独的redis实例 twemproxy：twemproxy集群 redis cluster：redis集群 rdb file：.rdb文件 aof file：.aof文件 servers：redis地址组，如果type:twemproxy，则为twemproxy配置文件，如果type:rdb file，则为rdb文件名。 redis_auth：连接redis服务的认证auth。 timeout：读写redis服务的超时时间(ms)，默认为120000ms hash：哈希方法名。仅当type:twemproxy有效。可以为one_at_a_time、md5、crc16、crc32、crc32a、fnv1_64、fnv1a_64、fnv1_32、fnv1a_32、hsieh、murmur、jenkins。 hash_tag：用来哈希的关键key的两个字符，例如”{}” 或 “$$”。仅当type:twemproxy有效。只要标签内的关键key是相同的，能够将不同的键映射到同一服务器。 distribution：键的分布模式。仅当type:twemproxy有效。可以为 ketama、modula、random。 [common]： listen：监听的地址和端口。默认为127.0.0.1:8888 max_clients：可监听端口的最大连接数。默认为100 threads：工具可用的最多线程数。默认为cpu内核数。 step：解析请求的步数。默认为1，数字越大，迁移越快，需要越多的内存。 mbuf_size：请求的缓存大小（M），默认为512M noreply：是否检查目标组的回复，默认为false source_safe：是否保护源组机器的内存安全。默认为true，工具将允许在源组的同一台机器同时只有一个redis生成.rdb。 dir：工作目录。用来存储文件，例如rdb文件，默认为当前目录。 filter：过滤不符合表达式的Key，默认为NULL，支持通配符为glob-style风格 ? ：1个任意字符。例如 h?llo 匹配 hello, hallo , hxllo * ：0个或多个任意字符。例如 h*llo 匹配 hllo ， heeeello [characters]：匹配任意一个方括号内的字符，比如[abc]，要么匹配a，要么匹配b，要么匹配c。例如 h[ae]llo 匹配 hello ， hallo, 但不匹配 hillo。 [^character]：排除方括号内的字符。例如h[^e]llo 匹配 hallo, hbllo, … 但不匹配 hello。 [character-character]：表示2个字符范围内的都可以匹配，如[a-z]，[0-9]。例如h[a-b]llo 匹配 hallo 和 hbllo。 \用来转移特殊字符。 配置文件示例：（1）从单一实例迁移数据到twemproxy集群（single to twemproxy）：12345678910111213141516171819202122232425[source]type: singleservers: - 127.0.0.1:6379 - 127.0.0.1:6380 - 127.0.0.1:6381 - 127.0.0.1:6382[target]type: twemproxyhash: fnv1a_64hash_tag: &quot;&#123;&#125;&quot;distribution: ketamaservers: - 127.0.0.1:6380:1 server1 - 127.0.0.1:6381:1 server2 - 127.0.0.1:6382:1 server3 - 127.0.0.1:6383:1 server4[common]listen: 0.0.0.0:8888threads: 2step: 1mbuf_size: 1024source_safe: true （2）从twemproxy集群迁移数据到redis集群（twemproxy to redis cluster）1234567891011121314151617181920[source]type: twemproxyhash: fnv1a_64hash_tag: &quot;&#123;&#125;&quot;distribution: ketamaservers: - 127.0.0.1:6379 - 127.0.0.1:6380 - 127.0.0.1:6381 - 127.0.0.1:6382[target]type: redis clusterservers: - 127.0.0.1:7379[common]listen: 0.0.0.0:8888step: 1mbuf_size: 512 （3）从一个redis集群迁移数据到另一个集群（redis cluster to another redis cluster），配置filter为以”abc”开始的键12345678910111213[source]type: redis clusterservers: - 127.0.0.1:8379[target]type: redis clusterservers: - 127.0.0.1:7379[common]listen: 0.0.0.0:8888filter: abc* （4）从.rdb文件导入数据到redis集群（rdb file to redis cluster）12345678910111213141516[source]type: rdb fileservers: - /data/redis/dump1.rdb - /data/redis/dump2.rdb[target]type: redis clusterservers: - 127.0.0.1:7379[common]listen: 0.0.0.0:8888step: 2mbuf_size: 512source_safe: false （5）保存redis集群的数据到.rdb（redis cluster to rdb file）1234567891011[source]type: redis clusterservers: - 127.0.0.1:7379[target]type: rdb file[common]listen: 0.0.0.0:8888source_safe: true （6）从.aof文件导入数据到redis集群（aof file to redis cluster）1234567891011121314[source]type: aof fileservers: - /data/redis/appendonly1.aof - /data/redis/appendonly2.aof[target]type: redis clusterservers: - 127.0.0.1:7379[common]listen: 0.0.0.0:8888step: 2 （7）从redis集群迁移数据到单一实例（redis cluster to single）123456789101112[source]type: redis clusterservers: - 127.0.0.1:8379[target]type: singleservers: - 127.0.0.1:6379[common]listen: 0.0.0.0:8888 （8）从单一实例迁移数据到redis集群（single to redis cluster）123456789101112[source]type: singleservers: - 127.0.0.1:6379[target]type: redis clusterservers: - 127.0.0.1:7379[common]listen: 0.0.0.0:8888 监听redis-migrate-tool可以使用redis-cli连接工具，监听地址和端口设置在配置文件的[common]下的listen，默认为127.0.0.1:8888 1. info指令1234567891011121314151617181920212223242526272829303132333435363738394041$redis-cli -h 127.0.0.1 -p 8888127.0.0.1:8888&gt; info# Serverversion:0.1.0 # 工具的版本号os:Linux 2.6.32-573.12.1.el6.x86_64 x86_64 # 操作系统信息multiplexing_api:epoll # 多路复用接口gcc_version:4.4.7 # gcc版本process_id:9199 # 工具的进程idtcp_port:8888 # 工具监听的tcp端口号uptime_in_seconds:1662 # 工具运行的时间（秒）uptime_in_days:0 # 工具运行的时间（天）config_file:/ect/rmt.conf # 工具运行的配置文件名称# Clientsconnected_clients:1 # 当前连接的客户端数max_clients_limit:100 # 客户端同时连接最大限制total_connections_received:3 # 至今总共连接# Memorymem_allocator:jemalloc-4.0.4# Groupsource_nodes_count:32 # 源redis组的节点数target_nodes_count:48 # 目的redis组的节点数# Statsall_rdb_received:1 # 是否已接收源redis组节点的所有.rdb文件all_rdb_parsed:1 # 是否已解析源redis组节点的所有.rdb文件all_aof_loaded:0 # 是否已加载源redis组节点的所有.aof文件rdb_received_count:32 # 已接收的源redis组节点.rdb文件数rdb_parsed_count:32 # 已解析的源redis组节点.rdb文件数aof_loaded_count:0 # 已加载的源redis组节点.aof文件数total_msgs_recv:7753587 # 从源组节点接收的所有消息数total_msgs_sent:7753587 # 所有已发送目标节点并且收到的响应的消息数total_net_input_bytes:234636318 # 从源组接收的输入字节的总数total_net_output_bytes:255384129 # 已发送到目标组的输出字节的总数total_net_input_bytes_human:223.77M # 同total_net_input_bytes，而是转换成人类可读的。total_net_output_bytes_human:243.55M # 同total_net_output_bytes，而是转换成人类可读的。total_mbufs_inqueue:0 # 来自源组的mbufs输入缓存的命令数据(不包括rdb数据)total_msgs_outqueue:0 # 将被发送到目标组，和已被发送到目标，但正在等待响应的消息数127.0.0.1:8888&gt; 2. shutdown [seconds|asap] 执行指令后的行为： 停止从源redis复制 尝试将工具中的缓存数据发送到目标redis Redis-migrate-tool 停止，退出 参数： seconds：工具用于在退出之前将缓存的数据发送到目标redis的大多数时间。默认为10秒。 asap：不关心缓存的数据，立即退出。 例如，1234$ redis-cli -h 127.0.0.1 -p 8888127.0.0.1:8888&gt; shutdown 5OK(5.00s) 总结 不适用redis4.0.x及以上版本（高版本迁移小编会尽快出~） 当源中存在多库时，避免发生键值覆盖，最好换别的方式迁移 多源要不都不带密码，要不源是同一个密码，否则无法启动，在线变更密码可以通过config set requirepass [密码]]]></content>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis Cluster数据迁移单实例方案实战]]></title>
    <url>%2F2019%2F08%2F01%2FCluster%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB%E5%8D%95%E5%AE%9E%E4%BE%8B%E6%96%B9%E6%A1%88%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[前言Redis集群同步迁移的方法有很多，比如redis-port,redis-migrate-tool等工具，本文主要讲CentOS通过redis-migrate-tool从集群迁移到单实例。 redis-migrate-tool 是维品会开源的一款redis数据迁移工具，基于redis复制，快速，稳定，github地址为：https://github.com/vipshop/redis-migrate-tool 。 迁移工具的来源（source）可以是：单独的redis实例，twemproxy集群，redis cluster，rdb文件，aof文件。迁移工具的目标（target）可以是：单独的redis实例，twemproxy集群，redis cluster，rdb文件。目前测试的Redis版本为3.0.0，如果是4.0.x版本可能会报错12345[2019-07-31 16:57:50.393] rmt_redis.c:1517 MASTER &lt;-&gt; SLAVE sync: receiving 4860 bytes from master[172.16.255.34:7001@17001][2019-07-31 16:57:50.393] rmt_redis.c:1623 MASTER &lt;-&gt; SLAVE sync: RDB data for node[172.16.255.34:7001@17001] is received, used: 0 s[2019-07-31 16:57:50.393] rmt_redis.c:1643 rdb file node172.16.255.34:7001@17001-1564563470312707-6709.rdb write complete[2019-07-31 16:57:50.393] rmt_redis.c:6446 ERROR: Can&apos;t handle RDB format version -1549717496[2019-07-31 16:57:50.393] rmt_redis.c:6715 ERROR: Rdb file for node[172.16.255.34:7001@17001] parsed failed 如上Can&#39;t handle RDB format version -1549717496为版本兼容问题，各个版本的.rdb是不兼容的，rmt工具似乎还不支持4.0.X。 可考虑降版本，如果开启了appendonly yes，可通过.aof文件迁移。 当前集群数据存在 172.16.255.34:7000 172.16.255.34:7001172.16.255.35:7002 172.16.255.35:7003172.16.255.36:7004 172.16.255.36:7005 需要将集群的数据迁移到单实例的172.16.255.34:6379 安装依赖：1$ yum -y install automake libtool autoconf bzip2 git 安装redis-migrate-tool：12345$ git clone https://github.com/vipshop/redis-migrate-tool$ cd redis-migrate-tool$ autoreconf -fvi$ ./configure$ make 查看命令帮助可检查是否安装成功1$ src/redis-migrate-tool -h 如果出现123456789$ src/redis-migrate-tool -hThis is redis-migrate-tool-0.1.0Usage: redis-migrate-tool [-?hVdIn] [-v verbosity level] [-o output file] [-c conf file] [-C command] [-f source address] [-t target address] [-p pid file] [-m mbuf size] [-r target role] [-T thread number] [-b buffer size].... 安装成功。 更多介绍查看redis-migrate-tool使用详解 配置文件更改redis-migrate-tool默认的配置文件为rmt.conf，更改如下1234567891011121314$ vi rmt.conf[source]type: redis clusterservers :-172.16.255.34:7000[target]type: singleservers:-172.16.255.34:6379[common]listen: 0.0.0.0:8888 Esc :wq 保存退出。 source为集群，只需要配置一个servers就可以。 开始迁移1$ src/redis-migrate-tool -o log -d 开始在后台运行迁移，查看运行日志 tail -200 log。123....[2019-08-01 11:59:19.919] rmt_redis.c:6601 Rdb file for node[172.16.255.34:7000] parsed finished, use: 0 s.[2019-08-01 11:59:19.919] rmt_redis.c:6709 All nodes&apos; rdb file parsed finished for this write thread(0). 可看见All nodes&#39; rdb file parsed finished则表示运行完成。 运行数据检查1$ src/redis-migrate-tool -o log -d -C redis_check 默认抽样1000个数据检查12345678910Check job is running...Checked keys: 1000Inconsistent value keys: 0Inconsistent expire keys : 0Other check error keys: 0Checked OK keys: 1000All keys checked OK!Check job finished, used 1.041s 迁移成功~ 如果中途迁移失败，使用redis-cli -h 172.16.255.34，清除keys1172.16.255.34:6379&gt; flushall keys *检查是否清空，以免list等数据迁移时会增量，redis_check会检查一部分数据不一致，你会发现都是显示list1[2019-07-31 17:00:27.185] rmt_check.c:848 ERROR: key checked failed: check key&apos;s value error, value is inconsistent. key(len:15, type:list): 509A4C6A4WAY_46 Java使用JedisCluster连接集群的改成JedisPool12345678910@Overridepublic void afterPropertiesSet() throws Exception &#123; if (mJedis == null) &#123; mJedisPool = new JedisPool(genericObjectPoolConfig, hostName, Integer.valueOf(port), timeout); mJedis = mJedisPool.getResource(); &#125;&#125;@Autowiredprivate Jedis jedis;]]></content>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git出现RPC failed; curl 56 OpenSSL SSL_read: SSL_ERROR_SYSCALL, errfno 10054]]></title>
    <url>%2F2019%2F07%2F29%2Fgit%E5%87%BA%E7%8E%B0RPC-failed-curl-56-OpenSSL-SSL-read-SSL-ERROR-SYSCALL-errfno-10054%2F</url>
    <content type="text"><![CDATA[前言在Hexo博客增加本地文件资源下载之后，deploy发布时出现：123error: RPC failed; curl 56 OpenSSL SSL_read: SSL_ERROR_SYSCALL, errfno 10054fatal: The remote end hung up unexpectedlyEverything up-to-date 主要是因为一次性提交的文件过大，超出了HTTP缓存的上限。 解决 通过配置缓存可解决。1git config --global http.postBuffer 100M 查看配置git config --list]]></content>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy+Selenium爬取动态渲染网站]]></title>
    <url>%2F2019%2F07%2F17%2FScrapy-Selenium%E7%88%AC%E5%8F%96%E5%8A%A8%E6%80%81%E6%B8%B2%E6%9F%93%E7%BD%91%E7%AB%99%2F</url>
    <content type="text"><![CDATA[1.简介Selenium是一个用于Web应用程序测试的工具。直接运行在浏览器中，就像真正的用户在操作一样。支持的浏览器包括IE（7, 8, 9, 10, 11），Firefox，Safari，Chrome，Opera等，在爬虫上则是模拟正常用户访问网页并获取数据。 2.安装 Selenium1&gt; pip install selenium 3.安装驱动模拟真正的用户操作当然需要选择好用来操作的浏览器，根据浏览器来安装相应的驱动调起。 3.1 Chrome使用selenium驱动chrome浏览器需要下载chromedriver，而且chromedriver版本需要与chrome的版本对应，版本错误的话则会运行报错。 查看chrome的版本，可通过帮助 &gt; 关于Google Chrome(G)。 下载chromedriver可以通过淘宝镜像地址：http://npm.taobao.org/mirrors/chromedriver/ 。最新的镜像与Chrome同名，尽量选择版本相近的避免兼容问题，镜像下notes.txt可查看当前驱动支持的版本。 选择合适的版本下载，下载完解压将chromedriver.exe放在有设置环境变量的目录下，小编是放在python的安装目录下的，即python.exe所在的目录。 3.2 Firefox使用selenium驱动Firefox浏览器需要下载geckodriver，查看浏览器版本通过帮助 &gt; 关于 Firefox。 下载geckodriver可通过mozilla的仓库地址：https://github.com/mozilla/geckodriver/releases。 选择合适的版本下载，解压后geckodriver.exe同样也是放在python的安装目录下。 3.3 其它浏览器驱动下载Opera：http://npm.taobao.org/mirrors/operadriver/ IE：http://selenium-release.storage.googleapis.com/index.html （版本号要与selenium的版本一致，查看安装的selenium版本，可通过pip show selenium）如果没有vpn可能会打不开，可点击下载3.14.0版本的。 4.Selenium使用4.1 Chrome 配置123456options = webdriver.ChromeOptions()## 无界面# options.add_argument('--headless')driver = webdriver.Chrome(chrome_options=options)driver.set_window_size(1366, 768)driver.set_page_load_timeout(self.timeout) 注意Chrome可能需要管理员权限相关配置，小编习惯性用Firefox😂 4.2 Firefox 配置1234567# 实例化参数对象options = webdriver.FirefoxOptions()# 无界面# options.add_argument('--headless')driver = webdriver.Firefox(firefox_options=options)driver.set_window_size(1400, 700)driver.set_page_load_timeout(self.timeout) 4.3 不显示打开浏览器的界面有的时候我们不想要看到爬取的一步步操作，只关心结果，则可以在参数配置12# 无界面options.add_argument('--headless') 4.4 禁用浏览器弹窗不是页面弹窗，是浏览器设置里的弹窗。在打开浏览器时，使用参数配置关闭Firefox12options.set_preference('dom.webnotifications.enabled', False)options.set_preference('dom.push.enabled', False) Chrome123456prefs = &#123; 'profile.default_content_setting_values': &#123; 'notifications': 2 &#125;&#125;options.add_experimental_option('prefs', prefs) 4.5 driver属性和方法 页面加载 1driver.get("http://www.baidu.com") 关闭浏览器 12# 爬虫结束关闭浏览器driver.close() 获取当前url 1driver.current_url 刷新 1driver.refresh() 页面标题 1driver.title 页面渲染后的源码 1driver.page_source 获取窗口信息 1driver.get_window_rect() 获取当前窗口的x,y坐标和当前窗口的高度和宽度，如：{‘height’: 1366, ‘width’: 768, ‘x’: 0, ‘y’: 200} 设置 User Agent(Firefox为例) 123profile = webdriver.FirefoxProfile()profile.set_preference("general.useragent.override", "some UA string")driver = webdriver.Firefox(profile=profile) 执行js脚本 使用driver.execute_script([js脚本],*args)同步执行，如滑动到第一个class为cm-explain-bottom的元素位置。 12driver.execute_script( "document.getElementsByClassName('cm-explain-bottom')[0].scrollIntoView(true)") 异步执行使用driver.execute_async_script([js脚本],*args)，*argsw为执行js代码要传入的参数。 查找元素 返回一个WebElement对象。 通过id属性：element = driver.find_element_by_id(&quot;coolestWidgetEvah&quot;) 通过class属性：cheeses = driver.find_elements_by_class_name(&quot;cheese&quot;) 通过标签名：frame = driver.find_element_by_tag_name(&quot;iframe&quot;) 通过css选择器：cheese = driver.find_element_by_css_selector(&quot;#food span.dairy.aged&quot;) 通过name属性：cheese = driver.find_element_by_name(&quot;cheese&quot;) 通过xpath：inputs = driver.find_elements_by_xpath(&quot;//input&quot;) 通过链接文本（完全匹配）：cheese = driver.find_element_by_link_text(&quot;cheese&quot;) 通过链接文本（部分匹配）：cheese = driver.find_element_by_partial_link_text(&quot;cheese&quot;) 元素(WebElement)的属性和方法 标签下文本：element.text 点击：element.click() 表单提交：element.submit() 输入：element.send_keys(123) 示例： 123456789# 等待邮箱和密码可定位及登录按钮可提交，清空输入框，分别输入用户名密码点击提交按钮email = self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "#email")))passwd = self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "#pass")))submit = self.wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, '#loginbutton')))email.clear()passwd.clear()email.send_keys(user)passwd.send_keys(password)submit.click() Cookie操作 1234567891011121314151617driver.get("http://www.example.com")# 给当前url域设置cookie# name的值对应cookie key，value的值对应cookie valuedriver.add_cookie(&#123;'name':'key', 'value':'value', 'path':'/'&#125;)# 可选的属性# 'domain' -&gt; String,# 'secure' -&gt; Boolean,# 'expiry' -&gt; Milliseconds since the Epoch it should expire.# 输出当前url所有的Cookiefor cookie in driver.get_cookies(): print "%s -&gt; %s" % (cookie['name'], cookie['value'])# 通过name删除Cookiedriver.delete_cookie("CookieName")# 删除所有的Cookiedriver.delete_all_cookies() 切换页面框架或窗口 1driver.switch_to.window("windowName") 切换默认框架：driver.switch_to.default_content() 切换最新窗口： 123windows = driver.window_handles# 切换到最新打开的窗口中switch_to.window(windows[-1]) 获取最新的alert弹窗 123alert = driver.switch_to.alert# 关闭弹窗alert.dismiss() 当前的url返回或者跟进 12driver.forward()driver.back() 截屏 123456# 返回页面的base64编码字符串base64 = driver.get_screenshot_as_base64()# 返回保存到文件的结果result = driver.get_screenshot_as_file("D:\\example.png")# png格式的二进制字符串pngSrc = driver.get_screenshot_as_png() 使用Selenium爬取七麦数据APP排行榜：点击前往]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Scrapy-爬取有道翻译的单词释义]]></title>
    <url>%2F2019%2F07%2F16%2FScrapy-%E7%88%AC%E5%8F%96%E6%9C%89%E9%81%93%E7%BF%BB%E8%AF%91%E7%9A%84%E5%8D%95%E8%AF%8D%E9%87%8A%E4%B9%89%2F</url>
    <content type="text"><![CDATA[前言熟悉Scrapy框架后，我们手写第一个爬虫，爬取有道翻译的单词发音，发音文件链接，释义，例句。 需要先熟悉Scrapy框架的同学：点击学习 如单词proportion：有道翻译的详情连接为 http://dict.youdao.com/w/eng/proportion 。本篇文章爬取的内容结果：12345678910&#123;"example": [&#123;"en": "I seemed to have lost all sense of proportion.", "zh": "我好象已经丧失了有关比例的一切感觉。"&#125;, &#123;"en": "The price of this article is out of(all) proportion to its value.", "zh": "这个商品的价格与它的价值完全不成比例。"&#125;, &#123;"en": "But, the use of interception bases on the violation of the citizen rights, so it should be satisfactory of the principle of legal reservation and the principle of proportion.", "zh": "但是，监听的适用是以侵害公民权利为前提的，因此监听在刑事侦查中的运用必须满足法律保留原则和比例原则的要求。"&#125;], "explain": ["n. 比例，占比；部分；面积；均衡", "vt. 使成比例；使均衡；分摊"], "pron": "[prə'pɔːʃ(ə)n]", "pron_url": "http://dict.youdao.com/dictvoice?audio=proportion&amp;type=1", "word": "proportion"&#125; 创建项目在需要创建的目录下，1scrapy startproject youdaoeng 回车即可创建默认的Scrapy项目架构。 创建Item创建YoudaoengItem继承scrapy.Item，并定义需要存储的单词，发音，发音文件链接，释义，例句。12345678910111213141516import scrapyclass YoudaoengItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() # 单词 word = scrapy.Field() # 英式发音 pron = scrapy.Field() # 发音audio文件链接 pron_url = scrapy.Field() # 释义 explain = scrapy.Field() # 例句 example = scrapy.Field() 创建Spider在spiders目录下创建EngSpider.py，并创建class EngSpider，继承于Spider。1234567891011121314from scrapy import Spiderclass EngSpider(Spider): name = "EngSpider" # 允许访问的域 allowed_domains = ["dict.youdao.com"] start_urls = [ 'http://dict.youdao.com/w/eng/agree', 'http://dict.youdao.com/w/eng/prophet', 'http://dict.youdao.com/w/eng/proportion'] def parse(self, response): pass name：用于区别Spider，该名字必须是唯一的。 start_urls：Spider在启动时进行爬取的url列表，首先会爬取第一个。 def parse(self, response)：得到请求url后的response信息的解析方法。 有道翻译的网址为http://dict.youdao.com/ ，根据分析，查询英文单词结果后链接更改，如查询agree，跳转单词详情地址为http://dict.youdao.com/w/eng/agree 。所以几乎可以认为单词的详情页链接可以是http://dict.youdao.com/w/eng/ 拼接上单词本身，所以配置start_urls我们查询三个单词的释义详情。 解析解析用的Selectors选择器有多种方法： xpath(): 传入xpath表达式，返回该表达式所对应的所有节点的selector list列表 。 css(): 传入CSS表达式，返回该表达式所对应的所有节点的selector list列表. extract(): 序列化该节点为unicode字符串并返回list。 re(): 根据传入的正则表达式对数据进行提取，返回unicode字符串list列表。 下面我们用xpath()选择节点，xpath的语法可参考w3c的http://www.w3school.com.cn/xpath/xpath_nodes.asp 学习，需要熟悉语法、运算符、函数等。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def parse(self, response): box = response.xpath('//*[@id="results-contents"]') word = YoudaoengItem() # 简明释义 box_simple = box.xpath('.//*[@id="phrsListTab"]') # 判断查出来的字是否存在 if box_simple: # 单词 word['word'] = box_simple.xpath('.//h2[@class="wordbook-js"]//span[@class="keyword"]/text()').extract()[0] # 英式发音 word['pron'] = box_simple.xpath( './/h2[@class="wordbook-js"]//div[@class="baav"]//*[@class="phonetic"]/text()').extract()[0] # 发音链接 word['pron_url'] = "http://dict.youdao.com/dictvoice?audio=" + word['word'] + "&amp;type=1" # 释义 word['explain'] = [] temp = box_simple.xpath('.//div[@class="trans-container"]//ul//li/text()').extract() for item in temp: if len(item) &gt; 0 and not re.search(r'\n', item) and not re.match(r' ', item): print(item) word['explain'].append(item) # 例句 time.sleep(3) word['example'] = [] example_root = box.xpath('//*[@id="bilingual"]//ul[@class="ol"]/li') # 1.双语例句是否存在 if example_root: for li in example_root: en = "" for span in li.xpath('./p[1]/span'): if span.xpath('./text()').extract(): en += span.xpath('./text()').extract()[0] elif span.xpath('./b/text()').extract(): en += span.xpath('./b/text()').extract()[0] zh = str().join(li.xpath('./p[2]/span/text()').extract()).replace(' ', '') word['example'].append(dict(en=en.replace('\"', '\\"'), zh=zh)) # 2.柯林斯英汉双解大辞典的例句是否存在 elif box.xpath('//*[@id="collinsResult"]//ul[@class="ol"]//div[@class="examples"]'): example_root = box.xpath('//*[@id="collinsResult"]//ul[@class="ol"]//li') for i in example_root: if i.xpath('.//*[@class="exampleLists"]'): en = i.xpath( './/*[@class="exampleLists"][1]//div[@class="examples"]/p[1]/text()').extract()[0] zh = i.xpath( './/*[@class="exampleLists"][1]//div[@class="examples"]/p[2]/text()').extract()[0] word['example'].append(dict(en=en.replace('\"', '\\"'), zh=zh)) if len(word['example']) &gt;= 3: break yield word 最后 yield word则是返回解析的word 给Item Pipeline，进行随后的数据过滤或者存储。 运行爬虫-爬取单词释义运行爬虫，会爬取agree、prophet、proportion三个单词的详情，在项目目录下（scrapy.cfg所在的目录）1youdaoeng&gt;scrapy crawl EngSpider -o data.json 即可运行，窗口可以看见爬取的日志内容输出，运行结束后会在项目目录下生成一个data.json文件。 生成的数据为所有item的json格式数组，中文字符都是Unicode编码，可通过一些在线的json解析网站如 https://www.bejson.com/ ，Unicode转中文查看是我们想要的结果。 下载单词语音文件单词读音的mp3链接为解析时候保存的pron_url字段，接下来我们下载单词mp3文件到本地。在Item下增加属性pron_save_path，存储发音文件的本地地址：12# 发音 mp3 本地存放路径 pron_save_path = scrapy.Field() 并在settings.py文件中配置下载文件的目录，如在D:\scrapy_files\目录下，则配置1FILES_STORE = "D:\\scrapy_files\\" 增加ItemPipeline重新发起文件下载请求：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051class Mp3Pipeline(FilesPipeline): ''' 自定义文件下载管道 ''' def get_media_requests(self, item, info): ''' 根据文件的url发送请求（url跟进） :param item: :param info: :return: ''' # meta携带的数据可以在response获取到 yield scrapy.Request(url=item['pron_url'], meta=&#123;'item': item&#125;) def item_completed(self, results, item, info): ''' 处理请求结果 :param results: :param item: :param info: :return: ''' file_paths = [x['path'] for ok, x in results if ok] if not file_paths: raise DropItem("Item contains no files") # old_name = FILES_STORE + file_paths[0] # new_name = FILES_STORE + item['word'] + '.mp3' # 文件重命名 （相当于剪切） # os.rename(old_name, new_name) # item['pron_save_path'] = new_name # 返回的result是除去FILES_STORE的目录 item['pron_save_path'] = FILES_STORE + file_paths[0] return item def file_path(self, request, response=None, info=None): ''' 自定义文件保存路径 默认的保存路径是在FILES_STORE下创建的一个full来存放，如果我们想要直接在FILES_STORE下存放，则需要自定义存放路径。 默认下载的是无后缀的文件，需要增加.mp3后缀 :param request: :param response: :param info: :return: ''' file_name = request.meta['item']['word'] + ".mp3" return file_name 需要更改settings.py文件，配置Mp3Pipeline，后面的300为优先级，数字越大，优先级越低。123456# Configure item pipelines# See https://doc.scrapy.org/en/latest/topics/item-pipeline.htmlITEM_PIPELINES = &#123; # 'youdaoeng.pipelines.YoudaoengPipeline': 300, 'youdaoeng.pipelines.Mp3Pipeline': 300,&#125; 运行1youdaoeng&gt;scrapy crawl EngSpider -o data1.json 等待运行完成，则在项目目录下生成了data1.json，并在D:\scrapy_files\目录下生成了我们爬取的三个单词的释义。 项目源码]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Scrapy 爬取七麦 app数据排行榜]]></title>
    <url>%2F2019%2F07%2F16%2FScrapy-%E7%88%AC%E5%8F%96%E4%B8%83%E9%BA%A6-app%E6%95%B0%E6%8D%AE%E6%8E%92%E8%A1%8C%E6%A6%9C%2F</url>
    <content type="text"><![CDATA[前言熟悉Scrapy之后，本篇文章带大家爬取七麦数据（https://www.qimai.cn/rank ）的ios appstore付费应用排行榜前100名应用。 爬取内容包括app在列表中的下标，app图标地址，app的名称信息，app的类型，在分类中的排行，开发者，详情等。 考虑的问题： Forbidden by robots.txt的错误 网页返回403 页面通过动态渲染，普通的请求url，在页面渲染之前已经返回response，解析没有数据 列表一页20个app，想要拿到前100个需要翻页，但是翻页没有更改url，而是通过js动态加载 … 需要先熟悉Scrapy框架的同学：点击学习 创建项目在需要放置项目的目录下，1&gt; scrapy startproject qimairank 回车即可创建默认的Scrapy项目架构。 创建Item创建Item来存储我们爬取的app在列表中的下标，app图标地址，app的名称信息，app的类型，在分类中的排行，开发者，详情。修改items.py，在下面增加123456789101112131415class RankItem(scrapy.Item): # 下标 index = scrapy.Field() # 图标地址 src = scrapy.Field() # app标题信息 title = scrapy.Field() # app类型 type = scrapy.Field() # 分类中的排行 type_rank = scrapy.Field() # 开发者 company = scrapy.Field() # 详情信息 info = scrapy.Field() 创建Spider在spiders目录下创建RankSpider.py，并创建class RankSpider，继承于scrapy.Spider。12345678import scrapyclass RankSpider(scrapy.Spider): name = "RankSpider" start_urls = ["https://www.qimai.cn/rank"] def parse(self, response): pass name：用于区别Spider，该名字必须是唯一的。 start_urls：Spider在启动时进行爬取的url列表，首先会爬取第一个。 def parse(self, response)：得到url的response信息后的解析方法。 解析付费榜解析用的Selectors选择器有多种方法： xpath(): 传入xpath表达式，返回该表达式所对应的所有节点的selector list列表 。 css(): 传入CSS表达式，返回该表达式所对应的所有节点的selector list列表. extract(): 序列化该节点为unicode字符串并返回list。 re(): 根据传入的正则表达式对数据进行提取，返回unicode字符串list列表。 下面我们用xpath()选择节点，xpath的语法可参考w3c的http://www.w3school.com.cn/xpath/xpath_nodes.asp 学习，需要熟悉语法、运算符、函数等。1234567891011121314151617181920212223242526def parse(self, response): base = response.xpath( "//div[@class='ivu-row rank-all-item']/div[@class='ivu-col ivu-col-span-8'][2]//ul/li[@class='child-item']/div[@class='ivu-row']") for box in base: # 创建实例 rankItem = RankItem() # 下标 rankItem['index'] = \ box.xpath(".//div[@class='ivu-col ivu-col-span-3 left-item']/span/text()").extract()[0] # 图标地址 rankItem['src'] = box.xpath(".//img/@src").extract()[0] # app名称信息 rankItem['title'] = box.xpath(".//div[@class='info-content']//a/text()").extract()[0] # app类型 rankItem['type'] = box.xpath(".//div[@class='info-content']//p[@class='small-txt']/text()").extract()[0] # 分类中的排行 rankItem['type_rank'] = box.xpath( ".//div[@class='info-content']//p[@class='small-txt']//span[@class='rank-item']/text()").extract()[ 0] # 开发者 rankItem['company'] = box.xpath( ".//div[@class='info-content']//p[@class='small-txt']//span[@class='company-item']/text()").extract()[ 0] # 详情页地址 infoUrl = "https://www.qimai.cn" + box.xpath(".//div[@class='info-content']//a/@href").extract()[0] yield rankItem 运行爬取初始app列表直接运行1qimairank&gt;scrapy crawl RankSpider -o data.json 你会发现窗口没有item输出，data.json中也没有数据，是我们写错了吗？ scrapy默认遵守robot协议的，在访问网址前会先访问robot.txt来查看自己是否有权限访问。如果网站不允许被爬，就不能访问。怎么样不遵守协议呢？1234settings.py# Obey robots.txt rulesROBOTSTXT_OBEY = False 再次运行仍然失败，我们来看下具体原因：因为七麦网站对请求的User-Agent做了校验，解决办法是在配置文件123456789101112131415161718192021222324252627settings.py# Enable or disable downloader middlewares# See https://doc.scrapy.org/en/latest/topics/downloader-middleware.htmlDOWNLOADER_MIDDLEWARES = &#123; # 'qimairank.middlewares.QimairankDownloaderMiddleware': 543, 'qimairank.middlewares.RandomUserAgent': 1,&#125;USER_AGENTS = [ "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)", "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)", "Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)", "Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)", "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)", "Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)", "Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)", "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)", "Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6", "Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1", "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0", "Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5", "Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6", "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11", "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20", "Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52",] 并在middlewares.py中创建RandomUserAgent12345678910111213141516import randomclass RandomUserAgent(object): """ 随机获取settings.py中配置的USER_AGENTS设置'User-Agent' """ def __init__(self, agents): self.agents = agents @classmethod def from_crawler(cls, crawler): return cls(crawler.settings.getlist('USER_AGENTS')) def process_request(self, request, spider): request.headers.setdefault('User-Agent', random.choice(self.agents)) 再次运行，没有报错，但是没有数据，是我们的xpath写错啦？我们在parse中增加输出body的信息 可以看到body为空，没有我们需要的列表数据，这是因为七麦数据是通过js动态渲染的，在渲染完成前，我们的response已经返回，那么怎么样才能等一等呀，等到渲染完成才返回呢？ 爬取动态渲染的方式，我知道是通过Splash或者Selenium，像我们的桌面版系统可以选择用Selenium，操作可以设置可视化，所有界面操作都能看见，Splash依赖于Docker，无界面。 安装Selenium包：1pip install selenium 使用前需要安装驱动，配置详情点击 驱动安装完成，在middlewares.py中创建 SeleniumMiddleware123456789101112131415161718192021222324252627282930313233343536373839404142class SeleniumMiddleware(object): def __init__(self): self.timeout = 50 # 2.Firefox--------------------------------- # 实例化参数对象 options = webdriver.FirefoxOptions() # 无界面 # options.add_argument('--headless') # 关闭浏览器弹窗 options.set_preference('dom.webnotifications.enabled', False) options.set_preference('dom.push.enabled', False) # 打开浏览器 self.browser = webdriver.Firefox(firefox_options=options) # 指定浏览器窗口大小 self.browser.set_window_size(1400, 700) # 设置页面加载超时时间 self.browser.set_page_load_timeout(self.timeout) self.wait = WebDriverWait(self.browser, self.timeout) def process_request(self, request, spider): # 当请求的页面不是当前页面时 if self.browser.current_url != request.url: # 获取页面 self.browser.get(request.url) time.sleep(5) else: pass # 返回页面的response return HtmlResponse(url=self.browser.current_url, body=self.browser.page_source, encoding="utf-8", request=request) def spider_closed(self): # 爬虫结束 关闭窗口 self.browser.close() pass @classmethod def from_crawler(cls, crawler): # 设置爬虫结束的回调监听 s = cls() crawler.signals.connect(s.spider_closed, signal=signals.spider_closed) return s 在settins.py中配置123456# Enable or disable downloader middlewaresDOWNLOADER_MIDDLEWARES = &#123; # 'qimairank.middlewares.QimairankDownloaderMiddleware': 543, 'qimairank.middlewares.RandomUserAgent': 1, 'qimairank.middlewares.SeleniumMiddleware': 10,&#125; 再次运行scrapy crawl RankSpider -o data.json，啦啦啦~这回有数据啦。 Selenium调用JS脚本观察爬取出来的data.json，发现怎么肥四，只有20条数据，而且除了前6个的app图标都是七麦的默认图标。 这是因为七麦数据的列表默认每页20条，而且默认渲染前6个的图标，其余的页需要触发滑动事件加载，而且滑动到的图标才开始渲染。这样怎么办呢？我们只需要滑动到可以加载的按钮就可以啦，检查发现在三个列表的外层标签有一个class为cm-explain-bottom的标签我们用Selenium调用js脚本，滑动到这个标签就可以啦，在中间件process_request方法更改1234567891011121314151617def process_request(self, request, spider): # 当请求的页面不是当前页面时 if self.browser.current_url != request.url: # 获取页面 self.browser.get(request.url) time.sleep(5) # 请求的url开始为https://www.qimai.cn/rank/时，调用滑动界面，每页20个，滑动4次 if request.url.startswith("https://www.qimai.cn/rank"): try: for i in (0, 1, 2, 3): self.browser.execute_script( "document.getElementsByClassName('cm-explain-bottom')[0].scrollIntoView(true)") time.sleep(4) except JavascriptException as e: pass except Exception as e: pass 再次执行scrapy crawl RankSpider -o data1.json，则可看见已经生成data1.json里面有100个item。 获取app详情详情页需要跟进url，我们在RankSpider#parse方法中，不用yield Item，而是yield Request就可以跟进。12345# 详情页地址infoUrl = "https://www.qimai.cn" + box.xpath(".//div[@class='info-content']//a/@href").extract()[0]# yield rankItemyield Request(infoUrl.replace("rank", "baseinfo"), self.parseInfo, meta=&#123;'rankItem': dict(rankItem).copy()&#125;, dont_filter=True) 解析的infoUrl替换”rank”字符串为”baseinfo”就可以访问app应用信息页，用meta传递item到下一个解析方法中，用软拷贝的方式，避免Item因为地址相同，内容覆盖。 self.parseInfo为指定这次请求的解析方法，1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859def parseInfo(self, response): print("基地址：" + response.url) if response.status != 200: return rankItem = response.meta['rankItem'] info = dict() base = response.xpath("//div[@id='app-container']") if base.extract(): # try: # 描述 try: info['desc'] = base.xpath( ".//div[@class='app-header']//div[@class='app-subtitle']/text()").extract()[0] except Exception as e: print("无描述") # 开发商 info['auther'] = base.xpath( ".//div[@class='app-header']//div[@class='auther']//div[@class='value']/text()").extract()[0] # 分类 info['classify'] = base.xpath( ".//div[@class='app-header']//div[@class='genre']//div[@class='value']/a/text()").extract()[0] # appid info['appid'] = base.xpath( ".//div[@class='app-header']//div[@class='appid']//div[@class='value']/a/text()").extract()[0] # appstore地址 info['appstorelink'] = base.xpath( ".//div[@class='app-header']//div[@class='appid']//div[@class='value']/a/@href").extract()[0] # 价格 info['price'] = base.xpath( ".//div[@class='app-header']//div[@class='price']//div[@class='value']/text()").extract()[0] # 最新版本 info['version'] = base.xpath( ".//div[@class='app-header']//div[@class='version']//div[@class='value']/text()").extract()[0] # 应用截图 info['screenshot'] = base.xpath( ".//div[@class='router-wrapper']//div[@class='app-screenshot']//div[@class='screenshot-box']//img/@src").extract() # 应用描述 info['desc'] = base.xpath( ".//div[@class='router-wrapper']//div[@class='app-describe']//div[@class='description']").extract()[ 0] # 应用基本信息 info['baseinfo'] = [] for infoBase in base.xpath( ".//div[@class='router-wrapper']//div[@class='app-baseinfo']//ul[@class='baseinfo-list']/li"): # print(info['baseinfo']) try: info['baseinfo'].append(dict(type=infoBase.xpath(".//*[@class='type']/text()").extract()[0], info=infoBase.xpath(".//*[@class='info-txt']/text()").extract()[0])) except Exception as e: pass rankItem['info'] = info # 替换图标 列表加载为默认图标 rankItem['src'] = \ response.xpath("//*[@id='app-side-bar']//div[@class='logo-wrap']/img/@src").extract()[ 0] yield rankItem 再次执行scrapy crawl RankSpider -o data1.json，则可看见已经生成data2.json，但是生成的列表不是排行的列表，甚至是乱序的，原因是因为我们使用了url跟进返回，每个页面的请求返回的速度不一样，需要排序的话就写个小脚本按照index排个序。 项目源码]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hexo--第二弹]]></title>
    <url>%2F2019%2F07%2F10%2FHexo-%E7%AC%AC%E4%BA%8C%E5%BC%B9%2F</url>
    <content type="text"><![CDATA[目录 1. Hexo支持流程图、时序图 2. Hexo多行代码提供复制 3. Hexo复制时追加版权 Hexo支持流程图、时序图画流程图还需要用别的编辑器画了用图片导入？Hexo实现手写流程图也很简单哦，但是有个小坑，小编被坑了好久，接下来手把手👇带你们过坑。 markdown语法实现流程图的方式可以通过mermaid或flowchart，时序图则可以mermaid或sequence，但是默认是不会识别语法的，只是当做普通的多行代码，需要安装插件。 方式一：mermaid支持流程图（graph）、时序图（sequenceDiagram）、甘特图（gantt），可以说支持很多了。配置教方式二麻烦一点。 在线编辑器地址：https://mermaidjs.github.io/mermaid-live-editor/ ，可以利用在线编辑器编辑完流程图之后，下载SVG或者直接link。 安装官方说的是通过yarn安装（如果没有安装yarn，使用npm install -g yarn安装）1$ yarn add hexo-filter-mermaid-diagrams 也可以使用npm：1$ npm i hexo-filter-mermaid-diagrams 插件的官方网址 配置（1）修改站点配置文件_config.yml在最后加入123456# mermaid chart mermaid: ## mermaid url https://github.com/knsv/mermaid enable: true # default true version: "7.1.2" # default v7.1.2 options: # find more api options from https://github.com/knsv/mermaid/blob/master/src/mermaidAPI.js #startOnload: true // default true （2）Next主题更改：在themes/next/_partials/footer.swig 最后加入12345678&#123;% if theme.mermaid.enable %&#125; &lt;script src='https://unpkg.com/mermaid@&#123;&#123; theme.mermaid.version &#125;&#125;/dist/mermaid.min.js'&gt;&lt;/script&gt; &lt;script&gt; if (window.mermaid) &#123; mermaid.initialize(&#123;theme: 'default'&#125;); &#125; &lt;/script&gt;&#123;% endif %&#125; 主题可更改，包含 default | forest 重新hexo clean &amp;&amp; hexo generate &amp;&amp; hexo server --debug启动渲染也生效了。 示例1. 流程图示例 ```mermaidgraph TB start(开始)–&gt;inputA[输入用户名密码] inputA–&gt;opA{数据库查询子类} opA–&gt;conditionA{是否有此用户} conditionA–yes–&gt;conditionB{密码是否正确} conditionA–no–&gt;inputA conditionB–yes–&gt;opB[读入用户信息] conditionB–no–&gt;inputA opB–&gt;en(登录) ``` mermaid流程图展示： graph TB start(开始)-->inputA[输入用户名密码] inputA-->opA{数据库查询子类} opA-->conditionA{是否有此用户} conditionA--yes-->conditionB{密码是否正确} conditionA--no-->inputA conditionB--yes-->opB[读入用户信息] conditionB--no-->inputA opB-->en(登录) 2. 时序图示例 ```mermaidsequenceDiagramparticipant Clientparticipant Server Note left of Client:SYN_SENTClient-&gt;Server:SYN=1 seq=xNote right of Server:SYN_RCVDServer-&gt;Client:SYN=1 seq=y ACK=x+1Note left of Client:ESTABLISHEDClient-&gt;Server:ACK=y+1Note right of Server:ESTABLISHED``` mermaid时序图展示： sequenceDiagram participant Client participant Server Note left of Client:SYN_SENT Client->Server:SYN=1 seq=x Note right of Server:SYN_RCVD Server->Client:SYN=1 seq=y ACK=x+1 Note left of Client:ESTABLISHED Client->Server:ACK=y+1 Note right of Server:ESTABLISHED 要说的话mermaid帮助文档：https://mermaidjs.github.io/ ，可在里面查看更多的使用介绍及语法。 优点：颜色鲜艳；语法结构简单，不需要先声明；方向可指定；灵活，可以更改样式。 缺点：方块模式提供没有标准流程图的规范的形状，比如输入框的平行四边形是没有的，需要自定义；加载渲染较慢，会出现展示多行代码样式。···mermaidgraph LRid1&gt;id1]–&gt;id2[id2]id2—id3(id3)id3—|text|id4((id4))id4–&gt;|text|id5{id5} style id1 fill:#f9f,stroke:#333,stroke-width:4pxstyle id2 fill:#ccf,stroke:#f66,stroke-width:2px,stroke-dasharray: 5, 5··· graph LR id1>id1]-->id2[id2] id2---id3(id3) id3---|text|id4((id4)) id4-->|text|id5{id5} style id1 fill:#f9f,stroke:#333,stroke-width:4px style id2 fill:#ccf,stroke:#f66,stroke-width:2px,stroke-dasharray: 5, 5 更多流程图使用查看：https://mermaidjs.github.io/flowchart.html 流程图过长会占用界面大部分空间，博客中设置了最大高度，及居中展示，在themes/next/source/css/_custom/custom.styl下面加入12345/*mermaid图居中*/.mermaid&#123; text-align: center; max-height: 300px;&#125; 方式二：flowchart+sequence安装支持流程图，安装：1$ npm install --save hexo-filter-flowchart 支持时序图，安装：1$ npm install --save hexo-filter-sequence 配置（非必须）插件官方地址： flowchart sequence 官方配置提到需要更改站点配置文件_config.yml，增加：1234567891011121314flowchart: # raphael: # optional, the source url of raphael.js # flowchart: # optional, the source url of flowchart.js options: # options used for `drawSVG` sequence: # webfont: # optional, the source url of webfontloader.js # snap: # optional, the source url of snap.svg.js # underscore: # optional, the source url of underscore.js # sequence: # optional, the source url of sequence-diagram.js # css: # optional, the url for css, such as hand drawn theme options: theme: css_class: 亲测不配置也是可以的。hexo clean &amp;&amp; hexo generate &amp;&amp; hexo server --debug启动渲染也生效了。 示例1.流程图示例 ```flowst=&gt;start: 开始inputA=&gt;inputoutput: 输入用户名密码opA=&gt;operation: 数据库查询子类conditionA=&gt;condition: 是否有此用户conditionB=&gt;condition: 密码是否正确opB=&gt;operation: 读入用户信息e=&gt;end: 登录st-&gt;inputA-&gt;opA-&gt;conditionAconditionA(yes)-&gt;conditionBconditionA(no)-&gt;inputAconditionB(yes)-&gt;opB-&gt;econditionB(no)-&gt;inputA``` flowchart流程图展示： 2.时序图示例 ```sequenceparticipant Clientparticipant Server Note left of Client:SYN_SENTClient-&gt;Server:SYN=1 seq=xNote right of Server:SYN_RCVDServer-&gt;Client:SYN=1 seq=y ACK=x+1Note left of Client:ESTABLISHEDClient-&gt;Server:ACK=y+1Note right of Server:ESTABLISHED``` sequence时序图展示： 要说的话优点：标准流程图的样式展示；渲染快，几乎不会出现展示多行代码的时候；实现简单。 缺点：样式不能更改，黑白界面；流程图语法需要先声明后使用。 设置最大高度及居中展示，背景色，超出部分滑动：12345678910111213.flow-chart &#123; text-align: center; max-height: 300px; overflow: auto; background: #f7f7f7;&#125;.sequence &#123; text-align: center; max-height: 300px; overflow: auto; background: #f7f7f7;&#125; sequence的小编不走心，没有提供class，需要在node_modules/hexo-filter-sequence/lib/renderer.js修改，大约22行，设置id的时候同时增加class：1return start + '&lt;div id="' + seqId + '" class="sequence"&gt;&lt;/div&gt;' + end; 特别注意：很多人说sequence设置无效，需要更改依赖的snap为raphael，也有说更改站点配置文件的external_link为false，小编都试过了，无效。为啥子时序图还是失败了呢？小编搜了整个项目差点以为是跟motion.js里面的sequence重合有缺陷，都打算改插件了，然而并不需要！！如果您的使用的Hexo，而且时序图放在md文件的最后一个，导致渲染失效了的话，请在文章的最后输入一个回车，没错就是只需要一个回车就解决了。。不知道是不是Hexo的bug，所有的多行代码在文章末尾的都会出现渲染问题，并不是sequence的问题。 Hexo多行代码提供复制增加复制按钮及响应的js：1234567891011121314clipboard.jsvar initCopyCode = function () &#123; var copyHtml = ''; copyHtml += '&lt;button class="btn-copy" data-clipboard-snippet=""&gt;'; copyHtml += ' &lt;i class="fa fa-clipboard"&gt;&lt;/i&gt;&lt;span&gt;copy&lt;/span&gt;'; copyHtml += '&lt;/button&gt;'; $(".highlight .code pre").before(copyHtml); new ClipboardJS('.btn-copy', &#123; target: function (trigger) &#123; return trigger.nextElementSibling; &#125; &#125;); &#125; 资源下载：点击下载 下载完成后，将clipboard.js和clipboard-use.js放在 themes/next/source/js/src/下，并更改themes/next/layout/_layout.swig，在上面加入123&lt;!--代码块复制功能--&gt;&lt;script type='text/javascript' src='/js/src/clipboard.js'&gt;&lt;/script&gt;&lt;script type="text/javascript" src="/js/src/clipboard-use.js"&gt;&lt;/script&gt; 这样在鼠标在代码区域时右上角显示copy。 Hexo复制时追加版权虽然在主题配置文件_config.yml中更改post_copyright可以在文章底部增加版权声明信息，复制时并不能像很多博客网站一样复制时直接追加。 实现是通过监听copy事件，追加信息：123456789101112131415161718192021222324252627282930copyright.js(() =&gt; &#123; if (document.getElementsByClassName("post-copyright").length&gt;0) &#123; const author=document.getElementsByClassName("author")[0].textContent; document.addEventListener('copy', e =&gt; &#123; let clipboardData = e.clipboardData || window.clipboardData; if(!clipboardData) &#123; return; &#125; e.preventDefault(); const selection = window.getSelection().toString(); const textData = selection + '\n-----------------------\n' + (author ? `作者: $&#123;author&#125;\n` : '') + '原文: ' + window.location.href + '\n' + '版权声明：本博客所有文章除特别声明外，均采用 CC BY-NC-SA 3.0 许可协议。转载请注明出处！\n\n'; const htmlData = selection + '&lt;br/&gt;-----------------------&lt;br/&gt;' + (author ? `&lt;b&gt;作者&lt;/b&gt;: $&#123;author&#125;&lt;br/&gt;` : '') + `&lt;b&gt;原文&lt;/b&gt;: &lt;a href="$&#123;window.location.href&#125;"&gt;$&#123;window.location.href&#125;&lt;/a&gt;&lt;br/&gt;` + '版权声明：本博客所有文章除特别声明外，均采用 &lt;a href="https://creativecommons.org/licenses/by-nc-sa/3.0/"&gt;CC BY-NC-SA 3.0&lt;/a&gt; 许可协议。转载请注明出处！&lt;br/&gt;'; clipboardData.setData('text/html', htmlData); clipboardData.setData('text/plain', textData); &#125;); &#125;&#125;)(); 资源下载：copyright.js 点击下载下载完成后，copyright.js放在 themes/next/source/js/src/下，并更改themes/next/layout/_layout.swig，在上面加入12&lt;!--&#123;#复制版权申明#&#125;--&gt;&lt;script type="text/javascript" src="/js/src/copyright.js"&gt;&lt;/script&gt; 版权开启时，复制时即可增加版权信息。 st=>start: 开始 inputA=>inputoutput: 输入用户名密码 opA=>operation: 数据库查询子类 conditionA=>condition: 是否有此用户 conditionB=>condition: 密码是否正确 opB=>operation: 读入用户信息 e=>end: 登录 st->inputA->opA->conditionA conditionA(yes)->conditionB conditionA(no)->inputA conditionB(yes)->opB->e conditionB(no)->inputA{"theme":"simple","scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12} var code = document.getElementById("flowchart-0-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value)); var diagram = flowchart.parse(code); diagram.drawSVG("flowchart-0", options);participant Client participant Server Note left of Client:SYN_SENT Client->Server:SYN=1 seq=x Note right of Server:SYN_RCVD Server->Client:SYN=1 seq=y ACK=x+1 Note left of Client:ESTABLISHED Client->Server:ACK=y+1 Note right of Server:ESTABLISHED{"theme":"simple","scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12} var code = document.getElementById("sequence-0-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-0-options").value)); var diagram = Diagram.parse(code); diagram.drawSVG("sequence-0", options);]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Scrapy详解 爬虫框架入门看这一篇就够了！]]></title>
    <url>%2F2019%2F07%2F10%2FScrapy%E8%AF%A6%E8%A7%A3-%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%E5%85%A5%E9%97%A8%E7%9C%8B%E8%BF%99%E4%B8%80%E7%AF%87%E5%B0%B1%E5%A4%9F%E4%BA%86%EF%BC%81%2F</url>
    <content type="text"><![CDATA[前言学习Scrapy有一段时间了，当时想要获取一下百度汉字的解析，又不想一个个汉字去搜，复制粘贴太费劲，考虑到爬虫的便利性，这篇文章是介绍一个爬虫框架–Scrapy，非常主流的爬虫框架，写爬虫还不会Scrapy，你就out啦🙈~ 🐞爬虫的应用场景： 搜索多个汉字，存储下来汉字的解析 每隔一段时间获取一下最新天气，新闻等等 拿到豆瓣电影（豆瓣图书）的top100的电影名字、演员、上映时间以及各大网友的评论 需要下载网站的一系列图片，视频等，下载慕课网的课程视频 搜集安居客的所有房源，性价比分析 刷票、抢票 拿到微博当前的热门话题，自媒体需要即时写文章啦 … 架构官方解析：Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。 其最初是为了页面抓取（更确切来说,网络抓取）所设计的，也可以应用在获取API所返回的数据或者通用的网络爬虫。 架构分析： Scrapy Engine：Scrapy引擎。负责控制数据流在系统中所有组件中流动，并在相应动作发生时触发事件。 Scheduler：调度器。从Scrapy Engine接受请求（requests）并排序列入队列，并在引擎再次请求时返回。用它来决定下一个抓取的网址是什么，同时去除重复的网址。 Downloader：下载器。抓取网页并将网页内容返还给Spiders。建立在twisted异步模型。 Spiders：爬虫。用户自定义的类，主要用来解析网页，提取Items，发送url跟进等新请求等。 Item Pipelines：管道。主要用来处理Spider解析出来的Items，进行按规则过滤，验证，持久化存储（如数据库存储）等 Downloader Middlewares：下载中间件。位于Scrapy Engine和Downloader之间，主要是处理Scrapy引擎与下载器之间的请求及响应。 Spider Middlewares：爬虫中间件。位于Scrapy Engine和Spiders之间，主要工作是处理Spiders的响应输入和请求输出。 Scheduler Middlewares：调度中间件。位于Scrapy Engine和Scheduler之间。主要工作是处理从Scrapy Engine发送到Scheduler的请求和响应。 数据处理流程：1、引擎打开一个网站(open a domain)，找到处理该网站的Spider并向该Spider请求要爬取的第一个start_urls。2、引擎从Spider中获取到第一个要爬取的URL并在调度器(Scheduler)以Request调度。3、引擎向调度器请求下一个要爬取的URL。4、调度器返回下一个要爬取的URL给引擎，引擎将URL通过Downloader Middlewares（request）转发给下载器(Downloader)。5、一旦页面下载完毕，Downloader生成一个该页面的Response，并将其通过Downloader Middlewares(response)发送给引擎。6、引擎从Downloader中接收到Response并通过Spider Middlewares（request）发送给Spider处理。7、Spider处理Response并返回爬取到的Item及(跟进的)新的Request给引擎。8、引擎将(Spider返回的)爬取到的Item给Item Pipeline，将(Spider返回的)Request给调度器。9、系统重复2-9的操作，直到调度中没有更多地request，然后断开引擎与网站之间的联系。 安装依赖环境： Python 2.7及以上 Python Package: pip and setuptools. 现在 pip 依赖 setuptools ，如果未安装，则会自动安装 setuptools 。 使用pip安装:1pip install Scrapy 创建项目：1scrapy startproject [项目名] 如创建 scrapy startproject qimairank，会自动创建Scrapy的项目架构：1234567891011qimairank|--qimairank |--spiders |--__init__.py |--__init__.py |--items.py |--middlewares.py |--pipelines.py |--settings.py|--scrapy.cfg scrapy.cfg：项目的配置文件，指定settings文件，部署deploy的project名称等等。 qimairank：项目的python模块。 spiders：放置spider代码的目录。 items.py：项目中的item文件。 pipelines.py:项目中的pipelines文件。 middlewares.py：项目的中间件。 settings.py：Scrapy 配置文件。更多配置信息查看：https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/settings.html 第一个爬虫爬虫实现讲解： 第一弹：爬取有道翻译 👉 点击前往]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[排除aar/jar中冗余或者冲突包、类的脚本]]></title>
    <url>%2F2019%2F07%2F05%2F%E6%8E%92%E9%99%A4aar-jar%E4%B8%AD%E5%86%97%E4%BD%99%E6%88%96%E8%80%85%E5%86%B2%E7%AA%81%E5%8C%85%E3%80%81%E7%B1%BB%E7%9A%84%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[引入aar的冲突无所不在，通过远程依赖maven的包可以通过exclude关键字搭配module和group去除某个组，没办法去除具体的类。 那么如果是单独的aar包，想要排除aar下classes.jar包里的某个单独的包或者类怎么办？ 需要接入的jar包已经带了腾讯X5核心，当前依赖的已经包含X5核心，冲突又该如何解决呢？ 当前的gradle脚本（项目链接：https://github.com/luohongxfb/ExcludeAar ）可以解决。 目录 1 效果展示 2 如何使用 3 如何实现的 解压AAR/JAR包 按照排除规则对解压的jar重新打包(这个是重点) 重新打包成AAR包 1 效果展示如excludelib/libs/exampleAAR.aar，左边是未去除的包结构，右边是去除com.baidu之后的： 如excludelib/libs/exampleJAR.jar： 2 如何使用（1）将需要排除的aar或者jar包放在excludelib/libs下。 （2）更改excludelib/build.gradle123//需要排除的aar或者jar。（替换成需要排除的）artifacts.add("exclude", file('libs/exampleAAR.aar'))artifacts.add("exclude", file('libs/exampleJAR.jar')) （3）设置排除规则 如果您需要排除aar，那么请更改excludelib/excludeAar.gradle；如果您需要排除jar，那么请更改excludelib/excludeJar.gradle1234//需要排除的包名def excludePackages = ['com.baidu']//需要排除的类(需要全类名)def excludeClasses = [] （4）运行排除任务 排除后生成的aar在excludelib/build/excludeaar下，排除后生成的jar位于excludelib/build/excludejar。 然后就可以愉快的使用啦~ 3 如何实现的aar排除步骤： 1、获取到需要排除的原始AAR包 2、解压AAR包（zipTree配合Task Copy） 3、解压AAR包中的class.jar（zipTree配合Task Copy） 4、按照排除规则对解压的class.jar重新打包(Task Jar) 5、重新打包成AAR包(Task Zip) jar排除步骤 1、获取到需要排除的原始jar包 2、解压jar包（zipTree配合Task Copy） 3、按照排除规则对解压的jar重新打包(Task Jar) 解压AAR/JAR包123456task unZipAar(type: Copy) &#123; def zipFile = getDefaultAar() def outputDir = unZipAarFile from zipTree(zipFile) into outputDir&#125; 主要原理：zipTree配合Copy，实现解压。 Copy Task官方讲解：https://docs.gradle.org/current/dsl/org.gradle.api.tasks.Copy.html ziptree源码主要解析：创建一个新的file tree包含原来zip的内容，可以配合Copy实现解压。12345678910public interface Project&#123; /** * &lt;p&gt;Creates a new &#123;@code FileTree&#125; which contains the contents of the given ZIP file. * You can combine this method with the &#123;@link #copy(groovy.lang.Closure)&#125; * method to unzip a ZIP file * @param zipPath The ZIP file. Evaluated as per &#123;@link #file(Object)&#125;. * @return the file tree. Never returns null. */ FileTree zipTree(Object zipPath);&#125; 按照排除规则对解压的jar重新打包(这个是重点)1234567task zipJar(type: Jar) &#123; baseName = 'classes' from unZipJarFile destinationDir unZipAarFile exclude getExcludePackageRegex(excludePackages) exclude getExcludeClassRegex(excludeClasses)&#125; 这个步骤就是把之前解压的classes.jar文件，按照排除规则用Task Jar重新打包成jar文件。 Task Jar官方讲解：https://docs.gradle.org/current/dsl/org.gradle.jvm.tasks.Jar.html Property/Method Description baseName 压缩后的jar文件名。 from(sourcePaths) 需要压缩的目录。 destinationDir 压缩后存放的目录。 exclude(excludes) 需要排除的文件。 重新打包成AAR包12345678task excludeAar(type: Zip) &#123; group 'ex' description '生成一个排除之后的aar包' baseName excludeAarName extension "aar" from unZipAarFile destinationDir excludeAarFile&#125; 对classes.jar处理完成的aar重打包，主要用到Task Zip。 Task Zip官方讲解：https://docs.gradle.org/current/dsl/org.gradle.api.tasks.bundling.Zip.html Property/Method Description group setGroup(String group) 将当前的Task设置到指定组。 description setDescription(@Nullable String description) Task描述。 baseName 压缩后的aar文件名。 extension 压缩后的文件扩展名。 from(sourcePaths) 需要压缩的目录。 destinationDir 压缩后存放的目录。]]></content>
      <categories>
        <category>Android</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hexo--第一弹]]></title>
    <url>%2F2019%2F07%2F04%2FHexo-%E7%AC%AC%E4%B8%80%E5%BC%B9%2F</url>
    <content type="text"><![CDATA[目录 1. 不渲染.md或者layout 2. 新建文件草稿 1. 不渲染.md或者layoutHexo 在 source 文件夹下的所有 md 文件或者 html 文件都会被渲染，我们想给输出项目添加一个README.md文件，不想被渲染，保持markdown文件格式，不转换为html怎么办呢 不渲染html在头部添加Front-matter。.md还是会转换成.html，但是不会渲染样式。单纯只是标签。1234---layout: false--- .md还是被转换成了.html，但是没有渲染。想要自定义样式的.html文件可参考设置，但是还不符合我们的期望，我们想要的保持.md样式呀。 不渲染.md文件在站点配置文件_config.yml查找skip_render，设置需要跳过渲染的文件。 设置不被渲染的是source/README.md和source/test/a.html 1234#跳过渲染 相对目录是`source`目录skip_render: - README.md - test/a.html 在public目录可以看见我们的目的达到了~ 2. 新建文件草稿当我们有的文章没写完，或者暂时不想展示时，可以新建草稿。在项目目录下终端12//新建草稿$ hexo new draft &quot;草稿&quot; 在source/_drafts会生成一个 草稿.md文件，这个文件不会显示在页面上，链接也访问不到。想要渲染草稿需要在站点配置文件_config.yml查找render_drafts，设置为true。12//如果你希望强行预览草稿，更改配置文件：render_drafts: true 注意：改为true之后，deploy线上部署也会显示，不需要的时候务必改成false。将草稿发布，转换到source/_posts下：12//发布草稿$ hexo publish [草稿]]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[最完善的markdown转html/pdf方法、带目录生成]]></title>
    <url>%2F2019%2F07%2F04%2Fmarkdown%E8%BD%AChtml%E3%80%81pdf-%E5%B8%A6%E7%9B%AE%E5%BD%95%E7%94%9F%E6%88%90%2F</url>
    <content type="text"><![CDATA[最近小编需要写个SDK接入文档，用word调格式当然不如markdown来的舒服，写起来舒服了，但是.md没办法直接看效果，最终萌生出把markdown转成html形式的想法。 网上markdown转换html的方法有很多，包括在线转换和借助工具，最终小编选择了VisualStudio Code的插件Markdown Preview Enhanced。 目录 1.在线转换的网站 2.借助工具 2.1 typora 2.2 i5ting_toc 2.3 VisualStudio Code插件 插件1：copy Markdown as HTML 插件2：Markdown+Math 插件3：Markdown Preview Enhanced （推荐） 1.在线转换的网站 http://www.bejson.com/convert/html2markdown/ ：不支持表格，不推荐 http://www.atool9.com/html2markdown.php ：不支持表格，不推荐 https://www.zai17.com/md2html/ ：```{代码}```包含的代码处理不太理想，不推荐 https://github.com/markedjs/marked ：转换结果比较理想，渲染需更改。可以用Demo Page查看效果。渲染可以使用github-markdown-css，github风格。 …（在线的还有很多，反正小编没有找到理想的） 2.借助工具2.1 typora官网下载。很多人说好用。Q@Q原谅小编没使明白。 2.2 i5ting_toc解析渲染ok，可自动生成侧边目录，不可以编写过程中实时预览，生成的侧边栏不更改本身.md文件，附带的样式太多。依赖Node环境，通过npm install i5ting_toc -g安装，等待安装完成。 输入i5ting_toc -h回车查看帮助。 转换html，输入i5ting_toc -f [需要转换文件名].md默认在同级目录生成preview文件夹，打开preview下的同名.html文件或者直接i5ting_toc -o可预览。 2.3 VisualStudio Code插件需要先安装VisualStudio Code。右键.md选择打开方式为VisualStudio Code。下载插件转换。 插件1：copy Markdown as HTMLCtrl+Shift+P（MacOS：cmd+shift+p）呼出命令面板。输入markdown:copy as html选中，复制到剪切板，新建一个.html文件，粘贴修改。转换结果比较理想，渲染需更改 插件2：Markdown+MathCtrl+Shift+P（MacOS：cmd+shift+p）呼出命令面板。输入markdown:clip markdown+math to html选中，同样复制到剪切板，需要新建一个.html文件，粘贴修改。转换结果比较理想，渲染需更改 插件3：Markdown Preview Enhanced （推荐）历经千辛万苦，小编终于找到了懒人的福音，可以一键生成目录、输出html/pdf的强大markdown插件–Markdown Preview Enhanced。详细用法点击查看官网。 多种特性： 自动编辑器及预览滑动同步 可导出的格式：漂亮的html、pandoc、电子书、幻灯片、PDF、 PNG、JPEG 。。 支持LaTeX 数学、流程图 / 时序图 以及各种其他种类的图形，嵌入 LaTeX, 渲染 TikZ, Chemfig 等图形 支持自定义预览 CSS 生成目录。（TOC 生成） … 这里小编介绍一下目录生成及输出html。首先安装插件：点击侧边栏最后一个Extensions，在搜索框输入Markdown Preview Enhanced，点击Install等待安装完成。 打开需要转换的.md文件，右键选择打开同步预览。 生成目录：详细讲解 光标放置想要生成目录的输出位置，Ctrl+Shift+P（MacOS：cmd+shift+p）呼出命令面板，输入Markdown Preview Enhanced: Create Toc会生成一段类似然后Ctrl+s保存，就会生成目录。注意：必须要在预览打开的情况下生成操作，保存才能更新。 输出html：在右侧的预览界面，右键点击选择HTML-&gt;HTML(offline)。在当前.md文件的同级目录会生成一个同名.html文件，解析渲染都理想~ ——————本文结束—————— 生成的html是你想要的样式吗？或者你有更好的方法欢迎联系我 更多好文欢迎关注我的公众号：]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hexo--GitHub Pages部署]]></title>
    <url>%2F2019%2F03%2F22%2FHexo-GitHub-Pages%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[部署到GitHub Pages请先准备一个GitHub账号，没有的话，出门右转浏览器地址栏输入：https://github.com/ Sign Up开始注册 有Git账号的准备新建仓库：https://github.com/new Repository name建议填自己用户名Owner带上.github.io 注意两点： 仓库名为[Owner].github.io时才可以通过https://[Owner].github.io访问，如果不是则需要按照原来的仓库形式，即Git地址+用户名+仓库名 仓库权限必须为Public 在Hexo项目配置文件_config.yml中，拉到底部，找到deploy配置字段，1234567# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repository: #前面创建的仓库地址 branch: master message: #描述 运行hexo clean &amp;&amp; hexo generate &amp;&amp; hexo deploy，没设置全局用户名的可能需要输入Git用户名密码。 等待一段时间~ 完成后在仓库的Settings中，配置Source为master branch就能通过https://[Owner].github.io访问啦~ 配置域名访问在腾讯云域名（https://www.qcloud.com/act/event/dnspod_baidu?!preview ）或阿里云域名（https://wanwang.aliyun.com/domain/?spm=a2c4g.11174283.2.1.789dc8caQX0KM1 ）购买一个域名登陆管理后台，配置云解析，添加记录将想要的前缀配置CNAME规则到git域名[Owner].github.io在仓库的Settings中GitHub Pages的Custom domain填入配置的前缀+域名即可。Enforce HTTPS会强制域名访问的时候也是通过https。]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hexo--博客搭建]]></title>
    <url>%2F2019%2F03%2F22%2FHexo--%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[本站的博客搭建采用Hexo+NexT主题，部署在GitHub Pages。下面手把手带你搭建免费高效的博客系统~ 简介Hexo–快速、简洁且高效的博客框架，基于NodeJS环境。 超快速度Node.js 所带来的超快生成速度，让上百个页面在几秒内瞬间完成渲染。 支持 MarkdownHexo 支持 GitHub Flavored Markdown 的所有功能，甚至可以整合 Octopress 的大多数插件。 一键部署只需一条指令即可部署到 GitHub Pages, Heroku 或其他网站。 丰富的插件Hexo 拥有强大的插件系统，安装插件可以让 Hexo 支持 Jade, CoffeeScript。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。 安装安装前提： NodeJS环境：下载地址 Git：Windows | Mac 安装Hexo：1npm install -g hexo-cli 具体的安装过程的问题，这里不再过多提及。官方安装文档：https://hexo.io/zh-cn/docs/ 建站新建文件夹创建项目：123hexo init &lt;folder&gt;cd &lt;folder&gt;npm install 若需要在当前文件夹直接创建，在当前目录下hexo init。 Hexo项目目录结构：12345678.├── _config.yml├── package.json├── scaffolds├── source| ├── _drafts| └── _posts└── themes _config.yml：网站的配置信息。官方配置文档：https://hexo.io/zh-cn/docs/configuration 。 package.json：应用程序的信息。 scaffolds：模版文件夹。当您新建文章时，Hexo 会根据scaffold来建立文件。官方模板配置文档：https://hexo.io/zh-cn/docs/templates source：资源文件夹是存放用户资源的地方。除 _posts 文件夹之外，开头命名为 _ (下划线)的文件 / 文件夹和隐藏的文件将会被忽略。Markdown 和 HTML 文件会被解析并放到 public 文件夹，而其他文件会被拷贝过去。 themes：主题文件夹。Hexo 会根据主题来生成静态页面。 项目下的_config.yml为项目配置文件，主题下的_config.yml为主题配置文件。 执行1hexo generate &amp;&amp; hexo server --debug 即可在本地http://localhost:4000/ 查看默认的网站样式啦~ 命令新建网站项目1hexo init [folder] 如果没有设置 folder ，Hexo 默认在目前的文件夹建立网站。 新建文章1hexo new [layout] &lt;title&gt; 如果没有设置 layout 的话，默认使用 网站配置_config.yml 中的 default_layout 参数代替。 如果标题包含空格的话，请使用引号括起来。 也可以简写：hexo n [layout] &lt;title&gt; 生成静态文件1hexo generate -d, –deploy 文件生成后立即部署网站 -w, –watch 监视文件变动 也可以简写：hexo g 本地启动服务器1hexo server 默认情况下，访问网址为： http://localhost:4000/。 -p, –port 重设端口 -s, –static 只使用静态文件 -l, –log 启动日记记录，使用覆盖记录格式 也可以简写：hexo s 线上部署1hexo deploy -g, –generate 部署之前预先生成静态文件 也可以简写：hexo d 部署到GitHub Pages请看这篇文章~ 清除缓存文件和已生成的静态文件1hexo clean 清除缓存文件 (db.json) 和已生成的静态文件 (public)。 在某些情况（尤其是更换主题后），如果发现您对站点的更改无论如何也不生效，您可能需要运行该命令。 更改配置文件_config.yml后需要运行该命令。不然可能会报Unhandled rejection Error: ENOENT: no such file or directory, open &#39;**&#39;。 主题Hexo的默认主题是landscape，官方推荐主题：https://hexo.io/themes/ 本博客网站使用的主题是NexT.Pisces如需要v6及以上的，请看新仓库https://github.com/theme-next/hexo-theme-next 安装先从git将主题拉下来，在项目下1git clone https://github.com/iissnan/hexo-theme-next themes/next 在项目配置文件_config.yml中查找theme，修改值为theme: next。（注意next前的空格哦） 然后hexo clean &amp;&amp; hexo generate &amp;&amp; hexo server --debug就能看见next主题的默认样式啦 选择NexT.Pisces需要在themes/next下的主题配置文件_config.yml中查找scheme，修改值为scheme: Pisces再运行上面的重新启动，就能看到样式啦 安装Hexo和Next主题到此就结束啦~ 要说的话建议在项目下package.json中配置开发运行脚本和部署脚本，在最外层json中添加：1234"scripts": &#123; "dev": "hexo clean &amp;&amp; hexo generate &amp;&amp; hexo server --debug", "start": "hexo clean &amp;&amp; hexo generate &amp;&amp; hexo deploy" &#125;, 以后开发运行npm run dev就好啦。线上部署使用npm run start。]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Scrapyd使用详解]]></title>
    <url>%2F2019%2F03%2F13%2Fscrapyd%E4%BD%BF%E7%94%A8%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[前言Scrapyd通常作为守护进程运行，它侦听运行爬虫的请求，并为每个请求生成一个进程，该进程基本上执行:scrapy crawl [myspider]。 Scrapyd还并行运行多个进程，将它们分配到max_proc和max_proc_per_cpu选项提供的固定数量的插槽中，启动尽可能多的进程来处理负载。 除了调度和管理进程之外，Scrapyd还提供了一个JSON web服务来上载新的项目版本(作为egg)和调度爬虫。 Scrapyd官方文档 https://scrapyd.readthedocs.io/en/latest/index.html 使用详解安装1pip install scrapyd 依赖的库及版本： Python 2.7 or above Twisted 8.0 or above Scrapy 1.0 or above six 启动在项目目录下，输入scrapyd即可运行，默认地址为http://localhost:68001scrapyd 官方详细配置文档说明：https://scrapyd.readthedocs.io/en/latest/config.html 修改默认配置信息可以在项目下新建一个scrapyd.conf或者在scrapy.cfg中增加[scrapyd]：123456[scrapyd]# 网页和Json服务监听的IP地址，默认为127.0.0.1bind_address = 127.0.0.1# 监听的端口，默认为6800http_port = 6800debug = off 项目部署部署主要分为两步： 将项目打包成egg 将egg通过Scrapyd的addversion.json接口上传到目标服务器 官方的部署上传接口文档：https://scrapyd.readthedocs.io/en/latest/api.html#addversion-json 推荐使用Scrapyd-client中的scrapyd-deploy一步上传，关于Scrapyd-client介绍： Scrapyd-client is a client for Scrapyd. It provides the general scrapyd-client and the scrapyd-deploy utility which allows you to deploy your project to a Scrapyd server. Scrapy-client安装1pip install scrapyd-client]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy线上部署</tag>
        <tag>Scrapyd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[webstorm .swig文件语法高亮]]></title>
    <url>%2F2019%2F03%2F07%2Fwebstorm-swig%E6%96%87%E4%BB%B6%E8%AF%AD%E6%B3%95%E9%AB%98%E4%BA%AE%2F</url>
    <content type="text"><![CDATA[前言swig是nodeJS前端模板引擎，swig特点看这里。 语法高亮webstrom（1）在File&gt;Settings&gt;Plugins，选择下方 Browse repositories... （2）在编辑框输入 twig，点击 Twig Support，Install之后重启webstorm （3）重启后，打开File&gt;Settings&gt;Editor&gt;File Types，找到Twig，点击+号添加*.swig，apply后就能看到.swig都高亮啦~]]></content>
      <tags>
        <tag>.swig</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git出现The following problems have occurred when adding the files: *** is in submodule]]></title>
    <url>%2F2019%2F03%2F06%2Fgit%E5%87%BA%E7%8E%B0The-following-problems-have-occurred-when-adding-the-files-is-in-submodule%2F</url>
    <content type="text"><![CDATA[前言开始搭建博客的时候，使用到Next主题1git clone https://github.com/iissnan/hexo-theme-next themes/next clone下的项目内包含.git隐藏文件，删除后，更改主题的代码后提交报错，手动git add依然报错The following problems have occurred when adding the files: *** is in submodule 解决这是因为本来是其他git项目，需要清除缓存记录。1git rm -rf --cached themes/next 运行上面的代码之后，再git add就不会报错啦~]]></content>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android P报java.net.UnknownServiceException: CLEARTEXT communication to *** not permitted by netwo...]]></title>
    <url>%2F2019%2F03%2F06%2FAndroid-P%E6%8A%A5java-net-UnknownServiceException-CLEARTEXT-communication-to-not-permitted-by-netwo%2F</url>
    <content type="text"><![CDATA[原因由于 Android P 限制了明文流量的网络请求，非加密的流量请求都会被系统禁止掉。 如果当前应用的请求是 htttp 请求，而非 https ,这样就会导系统禁止当前应用进行该请求，如果 WebView 的 url 用 http 协议，同样会出现加载失败，https 不受影响。 为此，OkHttp3 做了检查，所以如果使用了明文流量，默认情况下，在 Android P 版本 OkHttp3 就抛出异常: CLEARTEXT communication to *** not permitted by network security policy 解决办法（任选一个）1.在 res 下新建一个 xml目录，然后创建一个名为：network_security_config.xml 文件 ，该文件内容如下：1234&lt;?xml version="1.0" encoding="utf-8"?&gt;&lt;network-security-config&gt; &lt;base-config cleartextTrafficPermitted="true" /&gt;&lt;/network-security-config&gt; 然后在 AndroidManifest.xml application 标签内应用上面的xml配置：android:networkSecurityConfig=&quot;@xml/network_security_config&quot;123456&lt;application android:name=".app.App" android:allowBackup="true" android:icon="@mipmap/ic_launcher" android:networkSecurityConfig="@xml/network_security_config" android:label="@string/app_name" android:theme="@style/AppTheme"&gt; 2.服务器和本地应用都改用 https 3.targetSdkVersion 降级回到 27]]></content>
      <categories>
        <category>Android</category>
      </categories>
      <tags>
        <tag>Android P</tag>
      </tags>
  </entry>
</search>
